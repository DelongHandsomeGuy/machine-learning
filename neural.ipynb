{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "414fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "54e9fabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  educational-num  \\\n",
       "0   39          State-gov   77516   Bachelors               13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors               13   \n",
       "2   38            Private  215646     HS-grad                9   \n",
       "\n",
       "        marital-status          occupation    relationship    race gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White   Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White   Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White   Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data and add column names\n",
    "data=pd.read_csv(\"./adult.data\",header=None)\n",
    "data.columns=['age', 'workclass', 'fnlwgt', 'education', 'educational-num','marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income']\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0e7f2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              32561 non-null  int64 \n",
      " 1   workclass        32561 non-null  object\n",
      " 2   fnlwgt           32561 non-null  int64 \n",
      " 3   education        32561 non-null  object\n",
      " 4   educational-num  32561 non-null  int64 \n",
      " 5   marital-status   32561 non-null  object\n",
      " 6   occupation       32561 non-null  object\n",
      " 7   relationship     32561 non-null  object\n",
      " 8   race             32561 non-null  object\n",
      " 9   gender           32561 non-null  object\n",
      " 10  capital-gain     32561 non-null  int64 \n",
      " 11  capital-loss     32561 non-null  int64 \n",
      " 12  hours-per-week   32561 non-null  int64 \n",
      " 13  native-country   32561 non-null  object\n",
      " 14  income           32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8cec8c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                False\n",
       "workclass          False\n",
       "fnlwgt             False\n",
       "education          False\n",
       "educational-num    False\n",
       "marital-status     False\n",
       "occupation         False\n",
       "relationship       False\n",
       "race               False\n",
       "gender             False\n",
       "capital-gain       False\n",
       "capital-loss       False\n",
       "hours-per-week     False\n",
       "native-country     False\n",
       "income             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b44da6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30162 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              30162 non-null  int64 \n",
      " 1   workclass        30162 non-null  object\n",
      " 2   fnlwgt           30162 non-null  int64 \n",
      " 3   education        30162 non-null  object\n",
      " 4   educational-num  30162 non-null  int64 \n",
      " 5   marital-status   30162 non-null  object\n",
      " 6   occupation       30162 non-null  object\n",
      " 7   relationship     30162 non-null  object\n",
      " 8   race             30162 non-null  object\n",
      " 9   gender           30162 non-null  object\n",
      " 10  capital-gain     30162 non-null  int64 \n",
      " 11  capital-loss     30162 non-null  int64 \n",
      " 12  hours-per-week   30162 non-null  int64 \n",
      " 13  native-country   30162 non-null  object\n",
      " 14  income           30162 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# delete lines with '?'\n",
    "data.replace(' ?', np.nan,inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "980e7201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing duplicates: 23\n",
      "After removing duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate rows\n",
    "print(\"Before removing duplicates:\", data.duplicated().sum())\n",
    "data = data[~data.duplicated()]\n",
    "print(\"After removing duplicates:\", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a559c0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " <=50K    22633\n",
       " >50K      7506\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bd776cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30139 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              30139 non-null  float64\n",
      " 1   workclass        30139 non-null  object \n",
      " 2   fnlwgt           30139 non-null  float64\n",
      " 3   education        30139 non-null  object \n",
      " 4   educational-num  30139 non-null  float64\n",
      " 5   marital-status   30139 non-null  object \n",
      " 6   occupation       30139 non-null  object \n",
      " 7   relationship     30139 non-null  object \n",
      " 8   race             30139 non-null  object \n",
      " 9   gender           30139 non-null  object \n",
      " 10  capital-gain     30139 non-null  float64\n",
      " 11  capital-loss     30139 non-null  float64\n",
      " 12  hours-per-week   30139 non-null  float64\n",
      " 13  native-country   30139 non-null  object \n",
      " 14  income           30139 non-null  object \n",
      "dtypes: float64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# apply data normalisation to int64 variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = StandardScaler()\n",
    "for col in numeric_cols:\n",
    "    data[col] = scaler.fit_transform(data[[col]])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f75bf693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042516</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>-1.062676</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.128996</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.145925</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880215</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>-1.007829</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.128996</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-2.332060</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033639</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.244669</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.108678</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.425206</td>\n",
       "      <td>11th</td>\n",
       "      <td>-1.225149</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.795183</td>\n",
       "      <td>Private</td>\n",
       "      <td>1.406572</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.128996</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>-0.871338</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.638926</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>0.736639</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.244996</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0.118670</td>\n",
       "      <td>Private</td>\n",
       "      <td>-0.335246</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>1.489450</td>\n",
       "      <td>Private</td>\n",
       "      <td>-0.358567</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>-1.252110</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.110688</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-1.747682</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>1.032523</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>0.928780</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.880305</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>-0.078031</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30139 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age          workclass    fnlwgt    education  educational-num  \\\n",
       "0      0.042516          State-gov -1.062676    Bachelors         1.128996   \n",
       "1      0.880215   Self-emp-not-inc -1.007829    Bachelors         1.128996   \n",
       "2     -0.033639            Private  0.244669      HS-grad        -0.440434   \n",
       "3      1.108678            Private  0.425206         11th        -1.225149   \n",
       "4     -0.795183            Private  1.406572    Bachelors         1.128996   \n",
       "...         ...                ...       ...          ...              ...   \n",
       "32556 -0.871338            Private  0.638926   Assoc-acdm         0.736639   \n",
       "32557  0.118670            Private -0.335246      HS-grad        -0.440434   \n",
       "32558  1.489450            Private -0.358567      HS-grad        -0.440434   \n",
       "32559 -1.252110            Private  0.110688      HS-grad        -0.440434   \n",
       "32560  1.032523       Self-emp-inc  0.928780      HS-grad        -0.440434   \n",
       "\n",
       "            marital-status          occupation    relationship    race  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   White   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "...                    ...                 ...             ...     ...   \n",
       "32556   Married-civ-spouse        Tech-support            Wife   White   \n",
       "32557   Married-civ-spouse   Machine-op-inspct         Husband   White   \n",
       "32558              Widowed        Adm-clerical       Unmarried   White   \n",
       "32559        Never-married        Adm-clerical       Own-child   White   \n",
       "32560   Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "\n",
       "        gender  capital-gain  capital-loss  hours-per-week  native-country  \\\n",
       "0         Male      0.145925     -0.218673       -0.078031   United-States   \n",
       "1         Male     -0.147502     -0.218673       -2.332060   United-States   \n",
       "2         Male     -0.147502     -0.218673       -0.078031   United-States   \n",
       "3         Male     -0.147502     -0.218673       -0.078031   United-States   \n",
       "4       Female     -0.147502     -0.218673       -0.078031            Cuba   \n",
       "...        ...           ...           ...             ...             ...   \n",
       "32556   Female     -0.147502     -0.218673       -0.244996   United-States   \n",
       "32557     Male     -0.147502     -0.218673       -0.078031   United-States   \n",
       "32558   Female     -0.147502     -0.218673       -0.078031   United-States   \n",
       "32559     Male     -0.147502     -0.218673       -1.747682   United-States   \n",
       "32560   Female      1.880305     -0.218673       -0.078031   United-States   \n",
       "\n",
       "       income  \n",
       "0       <=50K  \n",
       "1       <=50K  \n",
       "2       <=50K  \n",
       "3       <=50K  \n",
       "4       <=50K  \n",
       "...       ...  \n",
       "32556   <=50K  \n",
       "32557    >50K  \n",
       "32558   <=50K  \n",
       "32559   <=50K  \n",
       "32560    >50K  \n",
       "\n",
       "[30139 rows x 15 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "47fb18dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30139 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              30139 non-null  float64\n",
      " 1   workclass        30139 non-null  int64  \n",
      " 2   fnlwgt           30139 non-null  float64\n",
      " 3   education        30139 non-null  int64  \n",
      " 4   educational-num  30139 non-null  float64\n",
      " 5   marital-status   30139 non-null  int64  \n",
      " 6   occupation       30139 non-null  int64  \n",
      " 7   relationship     30139 non-null  int64  \n",
      " 8   race             30139 non-null  int64  \n",
      " 9   gender           30139 non-null  int64  \n",
      " 10  capital-gain     30139 non-null  float64\n",
      " 11  capital-loss     30139 non-null  float64\n",
      " 12  hours-per-week   30139 non-null  float64\n",
      " 13  native-country   30139 non-null  int64  \n",
      " 14  income           30139 non-null  int64  \n",
      "dtypes: float64(6), int64(9)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "# encoding of integer values for object categories\n",
    "class_le = LabelEncoder()\n",
    "cat_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "for col in cat_cols:\n",
    "    data[col] = class_le.fit_transform(data[col].values)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f422e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6028\n",
      "torch.Size([15])\n",
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "# changing dataframe to tensor\n",
    "X = torch.tensor(np.concatenate(data.values), dtype=torch.float)\n",
    "X = X.view(30139, 15)\n",
    "#spilt\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "# define the Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# loading the data\n",
    "#num_workers should be 0 or 1, otherwise DataLoader worker (pid(s) 91713, 91714) exited unexpectedly\n",
    "train_dataset_X = MyDataset(X_train)\n",
    "train_loader_X = DataLoader(dataset=train_dataset_X, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset_X = MyDataset(X_test)\n",
    "test_loader_X = DataLoader(dataset=test_dataset_X, batch_size=32, shuffle=True, num_workers=0)\n",
    "# test_dataset_y = MyDataset(y_test)\n",
    "# test_loader_y = DataLoader(dataset=test_dataset_y, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "#check the size\n",
    "print(len(test_dataset_X))\n",
    "sample = train_dataset_X[0]  \n",
    "sample2 = test_dataset_X[0]\n",
    "print(sample.shape)  \n",
    "print(sample2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "263436f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#define the model\n",
    "#construct the network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(14, 200), # input layer\n",
    "    nn.ReLU(), # activation functionLeaky\n",
    "    nn.Linear(200, 400), # first hidden layer\n",
    "    nn.LeakyReLU(), # activation function\n",
    "    nn.Linear(400, 400), # second hidden layer\n",
    "    nn.LeakyReLU(), # activation function\n",
    "    nn.Linear(400, 400), # third hidden layer\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(400, 2)) # output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "53050aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-5)\n",
    "# import torch.optim as optim\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.65)\n",
    "criterium = nn.CrossEntropyLoss()\n",
    "\n",
    "#define the compute_loss_validation function\n",
    "def compute_loss_validation(model):\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    batch_size = test_loader_X.batch_size\n",
    "    for i, batch in enumerate(test_loader_X):\n",
    "        xs = batch[:,:-1]\n",
    "#        xs = batch\n",
    "#        print(xs.shape)\n",
    "#         print(xs)\n",
    "        ys = batch[:,-1].clamp(max=1).long()\n",
    "\n",
    "        pred_ys = model(xs) # generate the predictions using the model\n",
    "        loss = criterium(pred_ys, ys) # evaluate the predictions using the cross entropy loss\n",
    "        tot_loss += loss.item() # get the number and sum it to the total loss\n",
    "\n",
    "        if (i+1) % batch_size == 0:\n",
    "            break\n",
    "\n",
    "    loss = tot_loss / batch_size # normalize the loss based on the number of testing examples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "46da74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/66fq_8ls2qvdfwvlvcw1hxgh0000gn/T/ipykernel_86887/10087694.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = torch.tensor(batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [20/753], Train Loss: 0.6106, Validation Loss: 0.5906\n",
      "Epoch [1/10], Step [40/753], Train Loss: 0.5575, Validation Loss: 0.5747\n",
      "Epoch [1/10], Step [60/753], Train Loss: 0.5585, Validation Loss: 0.6012\n",
      "Epoch [1/10], Step [80/753], Train Loss: 0.5333, Validation Loss: 0.6377\n",
      "Epoch [1/10], Step [100/753], Train Loss: 0.5625, Validation Loss: 0.5428\n",
      "Epoch [1/10], Step [120/753], Train Loss: 0.5556, Validation Loss: 0.5489\n",
      "Epoch [1/10], Step [140/753], Train Loss: 0.5514, Validation Loss: 0.5049\n",
      "Epoch [1/10], Step [160/753], Train Loss: 0.4882, Validation Loss: 0.5303\n",
      "Epoch [1/10], Step [180/753], Train Loss: 0.5118, Validation Loss: 0.5500\n",
      "Epoch [1/10], Step [200/753], Train Loss: 0.5194, Validation Loss: 0.5478\n",
      "Epoch [1/10], Step [220/753], Train Loss: 0.5092, Validation Loss: 0.4749\n",
      "Epoch [1/10], Step [240/753], Train Loss: 0.4749, Validation Loss: 0.4575\n",
      "Epoch [1/10], Step [260/753], Train Loss: 0.4914, Validation Loss: 0.4472\n",
      "Epoch [1/10], Step [280/753], Train Loss: 0.5145, Validation Loss: 0.5755\n",
      "Epoch [1/10], Step [300/753], Train Loss: 0.4544, Validation Loss: 0.4615\n",
      "Epoch [1/10], Step [320/753], Train Loss: 0.4928, Validation Loss: 0.5266\n",
      "Epoch [1/10], Step [340/753], Train Loss: 0.4845, Validation Loss: 0.4926\n",
      "Epoch [1/10], Step [360/753], Train Loss: 0.5179, Validation Loss: 0.4192\n",
      "Epoch [1/10], Step [380/753], Train Loss: 0.4196, Validation Loss: 0.6580\n",
      "Epoch [1/10], Step [400/753], Train Loss: 0.5141, Validation Loss: 0.4808\n",
      "Epoch [1/10], Step [420/753], Train Loss: 0.4689, Validation Loss: 0.5014\n",
      "Epoch [1/10], Step [440/753], Train Loss: 0.4855, Validation Loss: 0.4551\n",
      "Epoch [1/10], Step [460/753], Train Loss: 0.5295, Validation Loss: 0.4619\n",
      "Epoch [1/10], Step [480/753], Train Loss: 0.4140, Validation Loss: 0.4105\n",
      "Epoch [1/10], Step [500/753], Train Loss: 0.4758, Validation Loss: 0.4502\n",
      "Epoch [1/10], Step [520/753], Train Loss: 0.4764, Validation Loss: 0.4207\n",
      "Epoch [1/10], Step [540/753], Train Loss: 0.4581, Validation Loss: 0.4647\n",
      "Epoch [1/10], Step [560/753], Train Loss: 0.4877, Validation Loss: 0.4976\n",
      "Epoch [1/10], Step [580/753], Train Loss: 0.4490, Validation Loss: 0.4409\n",
      "Epoch [1/10], Step [600/753], Train Loss: 0.4567, Validation Loss: 0.4294\n",
      "Epoch [1/10], Step [620/753], Train Loss: 0.4450, Validation Loss: 0.4836\n",
      "Epoch [1/10], Step [640/753], Train Loss: 0.4565, Validation Loss: 0.4160\n",
      "Epoch [1/10], Step [660/753], Train Loss: 0.4495, Validation Loss: 0.4196\n",
      "Epoch [1/10], Step [680/753], Train Loss: 0.4414, Validation Loss: 0.4294\n",
      "Epoch [1/10], Step [700/753], Train Loss: 0.4519, Validation Loss: 0.4681\n",
      "Epoch [1/10], Step [720/753], Train Loss: 0.4251, Validation Loss: 0.4114\n",
      "Epoch [1/10], Step [740/753], Train Loss: 0.4158, Validation Loss: 0.4579\n",
      "Epoch [2/10], Step [20/753], Train Loss: 0.7457, Validation Loss: 0.4172\n",
      "Epoch [2/10], Step [40/753], Train Loss: 0.4282, Validation Loss: 0.4506\n",
      "Epoch [2/10], Step [60/753], Train Loss: 0.4599, Validation Loss: 0.4148\n",
      "Epoch [2/10], Step [80/753], Train Loss: 0.3958, Validation Loss: 0.3991\n",
      "Epoch [2/10], Step [100/753], Train Loss: 0.4461, Validation Loss: 0.4377\n",
      "Epoch [2/10], Step [120/753], Train Loss: 0.4442, Validation Loss: 0.4146\n",
      "Epoch [2/10], Step [140/753], Train Loss: 0.4265, Validation Loss: 0.4054\n",
      "Epoch [2/10], Step [160/753], Train Loss: 0.3992, Validation Loss: 0.5412\n",
      "Epoch [2/10], Step [180/753], Train Loss: 0.4553, Validation Loss: 0.4282\n",
      "Epoch [2/10], Step [200/753], Train Loss: 0.3918, Validation Loss: 0.5045\n",
      "Epoch [2/10], Step [220/753], Train Loss: 0.4288, Validation Loss: 0.4397\n",
      "Epoch [2/10], Step [240/753], Train Loss: 0.4259, Validation Loss: 0.5440\n",
      "Epoch [2/10], Step [260/753], Train Loss: 0.4378, Validation Loss: 0.3996\n",
      "Epoch [2/10], Step [280/753], Train Loss: 0.4544, Validation Loss: 0.4032\n",
      "Epoch [2/10], Step [300/753], Train Loss: 0.3905, Validation Loss: 0.4847\n",
      "Epoch [2/10], Step [320/753], Train Loss: 0.4742, Validation Loss: 0.4130\n",
      "Epoch [2/10], Step [340/753], Train Loss: 0.4279, Validation Loss: 0.4029\n",
      "Epoch [2/10], Step [360/753], Train Loss: 0.4132, Validation Loss: 0.4183\n",
      "Epoch [2/10], Step [380/753], Train Loss: 0.4390, Validation Loss: 0.4107\n",
      "Epoch [2/10], Step [400/753], Train Loss: 0.4624, Validation Loss: 0.4396\n",
      "Epoch [2/10], Step [420/753], Train Loss: 0.4236, Validation Loss: 0.3918\n",
      "Epoch [2/10], Step [440/753], Train Loss: 0.4364, Validation Loss: 0.4197\n",
      "Epoch [2/10], Step [460/753], Train Loss: 0.4026, Validation Loss: 0.3973\n",
      "Epoch [2/10], Step [480/753], Train Loss: 0.4434, Validation Loss: 0.4470\n",
      "Epoch [2/10], Step [500/753], Train Loss: 0.4119, Validation Loss: 0.3697\n",
      "Epoch [2/10], Step [520/753], Train Loss: 0.4353, Validation Loss: 0.3736\n",
      "Epoch [2/10], Step [540/753], Train Loss: 0.4006, Validation Loss: 0.4115\n",
      "Epoch [2/10], Step [560/753], Train Loss: 0.4405, Validation Loss: 0.4212\n",
      "Epoch [2/10], Step [580/753], Train Loss: 0.4488, Validation Loss: 0.4226\n",
      "Epoch [2/10], Step [600/753], Train Loss: 0.3748, Validation Loss: 0.3996\n",
      "Epoch [2/10], Step [620/753], Train Loss: 0.4431, Validation Loss: 0.4071\n",
      "Epoch [2/10], Step [640/753], Train Loss: 0.3717, Validation Loss: 0.3914\n",
      "Epoch [2/10], Step [660/753], Train Loss: 0.3728, Validation Loss: 0.4143\n",
      "Epoch [2/10], Step [680/753], Train Loss: 0.4526, Validation Loss: 0.3988\n",
      "Epoch [2/10], Step [700/753], Train Loss: 0.4335, Validation Loss: 0.3917\n",
      "Epoch [2/10], Step [720/753], Train Loss: 0.3965, Validation Loss: 0.5537\n",
      "Epoch [2/10], Step [740/753], Train Loss: 0.4155, Validation Loss: 0.4206\n",
      "Epoch [3/10], Step [20/753], Train Loss: 0.7013, Validation Loss: 0.4085\n",
      "Epoch [3/10], Step [40/753], Train Loss: 0.4145, Validation Loss: 0.4173\n",
      "Epoch [3/10], Step [60/753], Train Loss: 0.4138, Validation Loss: 0.4020\n",
      "Epoch [3/10], Step [80/753], Train Loss: 0.3726, Validation Loss: 0.3688\n",
      "Epoch [3/10], Step [100/753], Train Loss: 0.4355, Validation Loss: 0.4038\n",
      "Epoch [3/10], Step [120/753], Train Loss: 0.3689, Validation Loss: 0.3869\n",
      "Epoch [3/10], Step [140/753], Train Loss: 0.3789, Validation Loss: 0.4046\n",
      "Epoch [3/10], Step [160/753], Train Loss: 0.3888, Validation Loss: 0.3714\n",
      "Epoch [3/10], Step [180/753], Train Loss: 0.4322, Validation Loss: 0.4097\n",
      "Epoch [3/10], Step [200/753], Train Loss: 0.3897, Validation Loss: 0.3778\n",
      "Epoch [3/10], Step [220/753], Train Loss: 0.4063, Validation Loss: 0.3966\n",
      "Epoch [3/10], Step [240/753], Train Loss: 0.3543, Validation Loss: 0.3777\n",
      "Epoch [3/10], Step [260/753], Train Loss: 0.4061, Validation Loss: 0.4012\n",
      "Epoch [3/10], Step [280/753], Train Loss: 0.3936, Validation Loss: 0.3836\n",
      "Epoch [3/10], Step [300/753], Train Loss: 0.4123, Validation Loss: 0.3888\n",
      "Epoch [3/10], Step [320/753], Train Loss: 0.3917, Validation Loss: 0.3879\n",
      "Epoch [3/10], Step [340/753], Train Loss: 0.4429, Validation Loss: 0.3784\n",
      "Epoch [3/10], Step [360/753], Train Loss: 0.4092, Validation Loss: 0.4209\n",
      "Epoch [3/10], Step [380/753], Train Loss: 0.3783, Validation Loss: 0.3636\n",
      "Epoch [3/10], Step [400/753], Train Loss: 0.4035, Validation Loss: 0.3871\n",
      "Epoch [3/10], Step [420/753], Train Loss: 0.3974, Validation Loss: 0.3854\n",
      "Epoch [3/10], Step [440/753], Train Loss: 0.4251, Validation Loss: 0.4013\n",
      "Epoch [3/10], Step [460/753], Train Loss: 0.4037, Validation Loss: 0.4201\n",
      "Epoch [3/10], Step [480/753], Train Loss: 0.4229, Validation Loss: 0.3625\n",
      "Epoch [3/10], Step [500/753], Train Loss: 0.3647, Validation Loss: 0.3944\n",
      "Epoch [3/10], Step [520/753], Train Loss: 0.3955, Validation Loss: 0.4164\n",
      "Epoch [3/10], Step [540/753], Train Loss: 0.4140, Validation Loss: 0.3722\n",
      "Epoch [3/10], Step [560/753], Train Loss: 0.3558, Validation Loss: 0.3642\n",
      "Epoch [3/10], Step [580/753], Train Loss: 0.4195, Validation Loss: 0.4209\n",
      "Epoch [3/10], Step [600/753], Train Loss: 0.3860, Validation Loss: 0.3923\n",
      "Epoch [3/10], Step [620/753], Train Loss: 0.3740, Validation Loss: 0.4007\n",
      "Epoch [3/10], Step [640/753], Train Loss: 0.4007, Validation Loss: 0.3735\n",
      "Epoch [3/10], Step [660/753], Train Loss: 0.3737, Validation Loss: 0.4402\n",
      "Epoch [3/10], Step [680/753], Train Loss: 0.4030, Validation Loss: 0.3762\n",
      "Epoch [3/10], Step [700/753], Train Loss: 0.3947, Validation Loss: 0.3683\n",
      "Epoch [3/10], Step [720/753], Train Loss: 0.3483, Validation Loss: 0.3329\n",
      "Epoch [3/10], Step [740/753], Train Loss: 0.3725, Validation Loss: 0.3810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [20/753], Train Loss: 0.6078, Validation Loss: 0.3506\n",
      "Epoch [4/10], Step [40/753], Train Loss: 0.3623, Validation Loss: 0.4319\n",
      "Epoch [4/10], Step [60/753], Train Loss: 0.4000, Validation Loss: 0.3522\n",
      "Epoch [4/10], Step [80/753], Train Loss: 0.3686, Validation Loss: 0.3533\n",
      "Epoch [4/10], Step [100/753], Train Loss: 0.3251, Validation Loss: 0.3571\n",
      "Epoch [4/10], Step [120/753], Train Loss: 0.3986, Validation Loss: 0.3725\n",
      "Epoch [4/10], Step [140/753], Train Loss: 0.4012, Validation Loss: 0.3530\n",
      "Epoch [4/10], Step [160/753], Train Loss: 0.3980, Validation Loss: 0.3543\n",
      "Epoch [4/10], Step [180/753], Train Loss: 0.3757, Validation Loss: 0.3748\n",
      "Epoch [4/10], Step [200/753], Train Loss: 0.3524, Validation Loss: 0.3426\n",
      "Epoch [4/10], Step [220/753], Train Loss: 0.3885, Validation Loss: 0.4041\n",
      "Epoch [4/10], Step [240/753], Train Loss: 0.3911, Validation Loss: 0.3542\n",
      "Epoch [4/10], Step [260/753], Train Loss: 0.3499, Validation Loss: 0.3399\n",
      "Epoch [4/10], Step [280/753], Train Loss: 0.3702, Validation Loss: 0.3632\n",
      "Epoch [4/10], Step [300/753], Train Loss: 0.3539, Validation Loss: 0.3445\n",
      "Epoch [4/10], Step [320/753], Train Loss: 0.3976, Validation Loss: 0.3367\n",
      "Epoch [4/10], Step [340/753], Train Loss: 0.3517, Validation Loss: 0.4463\n",
      "Epoch [4/10], Step [360/753], Train Loss: 0.3785, Validation Loss: 0.3380\n",
      "Epoch [4/10], Step [380/753], Train Loss: 0.4067, Validation Loss: 0.3604\n",
      "Epoch [4/10], Step [400/753], Train Loss: 0.3715, Validation Loss: 0.3850\n",
      "Epoch [4/10], Step [420/753], Train Loss: 0.3983, Validation Loss: 0.3676\n",
      "Epoch [4/10], Step [440/753], Train Loss: 0.3819, Validation Loss: 0.3986\n",
      "Epoch [4/10], Step [460/753], Train Loss: 0.3628, Validation Loss: 0.3647\n",
      "Epoch [4/10], Step [480/753], Train Loss: 0.4257, Validation Loss: 0.3511\n",
      "Epoch [4/10], Step [500/753], Train Loss: 0.3779, Validation Loss: 0.3817\n",
      "Epoch [4/10], Step [520/753], Train Loss: 0.3696, Validation Loss: 0.3418\n",
      "Epoch [4/10], Step [540/753], Train Loss: 0.4104, Validation Loss: 0.3679\n",
      "Epoch [4/10], Step [560/753], Train Loss: 0.3807, Validation Loss: 0.3769\n",
      "Epoch [4/10], Step [580/753], Train Loss: 0.3913, Validation Loss: 0.3699\n",
      "Epoch [4/10], Step [600/753], Train Loss: 0.4036, Validation Loss: 0.3617\n",
      "Epoch [4/10], Step [620/753], Train Loss: 0.3769, Validation Loss: 0.3499\n",
      "Epoch [4/10], Step [640/753], Train Loss: 0.3338, Validation Loss: 0.3537\n",
      "Epoch [4/10], Step [660/753], Train Loss: 0.3674, Validation Loss: 0.3601\n",
      "Epoch [4/10], Step [680/753], Train Loss: 0.3644, Validation Loss: 0.3788\n",
      "Epoch [4/10], Step [700/753], Train Loss: 0.3715, Validation Loss: 0.3480\n",
      "Epoch [4/10], Step [720/753], Train Loss: 0.3588, Validation Loss: 0.3553\n",
      "Epoch [4/10], Step [740/753], Train Loss: 0.3896, Validation Loss: 0.3831\n",
      "Epoch [5/10], Step [20/753], Train Loss: 0.6131, Validation Loss: 0.3538\n",
      "Epoch [5/10], Step [40/753], Train Loss: 0.3497, Validation Loss: 0.3650\n",
      "Epoch [5/10], Step [60/753], Train Loss: 0.3743, Validation Loss: 0.3553\n",
      "Epoch [5/10], Step [80/753], Train Loss: 0.4204, Validation Loss: 0.3790\n",
      "Epoch [5/10], Step [100/753], Train Loss: 0.3646, Validation Loss: 0.3824\n",
      "Epoch [5/10], Step [120/753], Train Loss: 0.3716, Validation Loss: 0.3708\n",
      "Epoch [5/10], Step [140/753], Train Loss: 0.3627, Validation Loss: 0.3511\n",
      "Epoch [5/10], Step [160/753], Train Loss: 0.3624, Validation Loss: 0.3379\n",
      "Epoch [5/10], Step [180/753], Train Loss: 0.3498, Validation Loss: 0.3682\n",
      "Epoch [5/10], Step [200/753], Train Loss: 0.3544, Validation Loss: 0.3401\n",
      "Epoch [5/10], Step [220/753], Train Loss: 0.3318, Validation Loss: 0.3389\n",
      "Epoch [5/10], Step [240/753], Train Loss: 0.3765, Validation Loss: 0.3860\n",
      "Epoch [5/10], Step [260/753], Train Loss: 0.3712, Validation Loss: 0.3923\n",
      "Epoch [5/10], Step [280/753], Train Loss: 0.3825, Validation Loss: 0.4968\n",
      "Epoch [5/10], Step [300/753], Train Loss: 0.3703, Validation Loss: 0.3384\n",
      "Epoch [5/10], Step [320/753], Train Loss: 0.3464, Validation Loss: 0.3867\n",
      "Epoch [5/10], Step [340/753], Train Loss: 0.3716, Validation Loss: 0.3592\n",
      "Epoch [5/10], Step [360/753], Train Loss: 0.3343, Validation Loss: 0.3657\n",
      "Epoch [5/10], Step [380/753], Train Loss: 0.3892, Validation Loss: 0.3445\n",
      "Epoch [5/10], Step [400/753], Train Loss: 0.3775, Validation Loss: 0.3180\n",
      "Epoch [5/10], Step [420/753], Train Loss: 0.3090, Validation Loss: 0.3458\n",
      "Epoch [5/10], Step [440/753], Train Loss: 0.3678, Validation Loss: 0.3657\n",
      "Epoch [5/10], Step [460/753], Train Loss: 0.3746, Validation Loss: 0.3401\n",
      "Epoch [5/10], Step [480/753], Train Loss: 0.3753, Validation Loss: 0.3691\n",
      "Epoch [5/10], Step [500/753], Train Loss: 0.3713, Validation Loss: 0.3464\n",
      "Epoch [5/10], Step [520/753], Train Loss: 0.3559, Validation Loss: 0.3530\n",
      "Epoch [5/10], Step [540/753], Train Loss: 0.3855, Validation Loss: 0.3132\n",
      "Epoch [5/10], Step [560/753], Train Loss: 0.3264, Validation Loss: 0.3197\n",
      "Epoch [5/10], Step [580/753], Train Loss: 0.3610, Validation Loss: 0.3697\n",
      "Epoch [5/10], Step [600/753], Train Loss: 0.3754, Validation Loss: 0.3467\n",
      "Epoch [5/10], Step [620/753], Train Loss: 0.3727, Validation Loss: 0.3543\n",
      "Epoch [5/10], Step [640/753], Train Loss: 0.3586, Validation Loss: 0.3658\n",
      "Epoch [5/10], Step [660/753], Train Loss: 0.3543, Validation Loss: 0.3570\n",
      "Epoch [5/10], Step [680/753], Train Loss: 0.3896, Validation Loss: 0.4124\n",
      "Epoch [5/10], Step [700/753], Train Loss: 0.4247, Validation Loss: 0.3771\n",
      "Epoch [5/10], Step [720/753], Train Loss: 0.3834, Validation Loss: 0.4312\n",
      "Epoch [5/10], Step [740/753], Train Loss: 0.3765, Validation Loss: 0.3666\n",
      "Epoch [6/10], Step [20/753], Train Loss: 0.6207, Validation Loss: 0.3291\n",
      "Epoch [6/10], Step [40/753], Train Loss: 0.3259, Validation Loss: 0.3571\n",
      "Epoch [6/10], Step [60/753], Train Loss: 0.3450, Validation Loss: 0.3751\n",
      "Epoch [6/10], Step [80/753], Train Loss: 0.3757, Validation Loss: 0.3460\n",
      "Epoch [6/10], Step [100/753], Train Loss: 0.3755, Validation Loss: 0.3612\n",
      "Epoch [6/10], Step [120/753], Train Loss: 0.3774, Validation Loss: 0.3461\n",
      "Epoch [6/10], Step [140/753], Train Loss: 0.3134, Validation Loss: 0.3602\n",
      "Epoch [6/10], Step [160/753], Train Loss: 0.3543, Validation Loss: 0.4078\n",
      "Epoch [6/10], Step [180/753], Train Loss: 0.3509, Validation Loss: 0.3465\n",
      "Epoch [6/10], Step [200/753], Train Loss: 0.3771, Validation Loss: 0.3529\n",
      "Epoch [6/10], Step [220/753], Train Loss: 0.3731, Validation Loss: 0.3458\n",
      "Epoch [6/10], Step [240/753], Train Loss: 0.4032, Validation Loss: 0.3434\n",
      "Epoch [6/10], Step [260/753], Train Loss: 0.3719, Validation Loss: 0.3552\n",
      "Epoch [6/10], Step [280/753], Train Loss: 0.3635, Validation Loss: 0.3468\n",
      "Epoch [6/10], Step [300/753], Train Loss: 0.3260, Validation Loss: 0.3508\n",
      "Epoch [6/10], Step [320/753], Train Loss: 0.3559, Validation Loss: 0.3676\n",
      "Epoch [6/10], Step [340/753], Train Loss: 0.3499, Validation Loss: 0.3992\n",
      "Epoch [6/10], Step [360/753], Train Loss: 0.3851, Validation Loss: 0.3629\n",
      "Epoch [6/10], Step [380/753], Train Loss: 0.3785, Validation Loss: 0.3528\n",
      "Epoch [6/10], Step [400/753], Train Loss: 0.3617, Validation Loss: 0.3749\n",
      "Epoch [6/10], Step [420/753], Train Loss: 0.4046, Validation Loss: 0.3370\n",
      "Epoch [6/10], Step [440/753], Train Loss: 0.3679, Validation Loss: 0.3334\n",
      "Epoch [6/10], Step [460/753], Train Loss: 0.3654, Validation Loss: 0.3472\n",
      "Epoch [6/10], Step [480/753], Train Loss: 0.3714, Validation Loss: 0.3418\n",
      "Epoch [6/10], Step [500/753], Train Loss: 0.3727, Validation Loss: 0.3815\n",
      "Epoch [6/10], Step [520/753], Train Loss: 0.3675, Validation Loss: 0.3742\n",
      "Epoch [6/10], Step [540/753], Train Loss: 0.3612, Validation Loss: 0.3987\n",
      "Epoch [6/10], Step [560/753], Train Loss: 0.3450, Validation Loss: 0.3713\n",
      "Epoch [6/10], Step [580/753], Train Loss: 0.3335, Validation Loss: 0.4025\n",
      "Epoch [6/10], Step [600/753], Train Loss: 0.3680, Validation Loss: 0.3431\n",
      "Epoch [6/10], Step [620/753], Train Loss: 0.3371, Validation Loss: 0.3860\n",
      "Epoch [6/10], Step [640/753], Train Loss: 0.3517, Validation Loss: 0.3577\n",
      "Epoch [6/10], Step [660/753], Train Loss: 0.3574, Validation Loss: 0.3546\n",
      "Epoch [6/10], Step [680/753], Train Loss: 0.3497, Validation Loss: 0.3623\n",
      "Epoch [6/10], Step [700/753], Train Loss: 0.4268, Validation Loss: 0.3700\n",
      "Epoch [6/10], Step [720/753], Train Loss: 0.3293, Validation Loss: 0.3771\n",
      "Epoch [6/10], Step [740/753], Train Loss: 0.3452, Validation Loss: 0.3343\n",
      "Epoch [7/10], Step [20/753], Train Loss: 0.6525, Validation Loss: 0.3208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [40/753], Train Loss: 0.3514, Validation Loss: 0.3782\n",
      "Epoch [7/10], Step [60/753], Train Loss: 0.3558, Validation Loss: 0.3467\n",
      "Epoch [7/10], Step [80/753], Train Loss: 0.3467, Validation Loss: 0.3690\n",
      "Epoch [7/10], Step [100/753], Train Loss: 0.3529, Validation Loss: 0.3484\n",
      "Epoch [7/10], Step [120/753], Train Loss: 0.3609, Validation Loss: 0.3204\n",
      "Epoch [7/10], Step [140/753], Train Loss: 0.3154, Validation Loss: 0.3436\n",
      "Epoch [7/10], Step [160/753], Train Loss: 0.2823, Validation Loss: 0.3655\n",
      "Epoch [7/10], Step [180/753], Train Loss: 0.3525, Validation Loss: 0.3540\n",
      "Epoch [7/10], Step [200/753], Train Loss: 0.3488, Validation Loss: 0.3374\n",
      "Epoch [7/10], Step [220/753], Train Loss: 0.3994, Validation Loss: 0.3455\n",
      "Epoch [7/10], Step [240/753], Train Loss: 0.3638, Validation Loss: 0.3976\n",
      "Epoch [7/10], Step [260/753], Train Loss: 0.3697, Validation Loss: 0.3665\n",
      "Epoch [7/10], Step [280/753], Train Loss: 0.3740, Validation Loss: 0.3298\n",
      "Epoch [7/10], Step [300/753], Train Loss: 0.3800, Validation Loss: 0.3238\n",
      "Epoch [7/10], Step [320/753], Train Loss: 0.3211, Validation Loss: 0.3288\n",
      "Epoch [7/10], Step [340/753], Train Loss: 0.3744, Validation Loss: 0.3582\n",
      "Epoch [7/10], Step [360/753], Train Loss: 0.3940, Validation Loss: 0.3782\n",
      "Epoch [7/10], Step [380/753], Train Loss: 0.3667, Validation Loss: 0.3563\n",
      "Epoch [7/10], Step [400/753], Train Loss: 0.3774, Validation Loss: 0.3615\n",
      "Epoch [7/10], Step [420/753], Train Loss: 0.3477, Validation Loss: 0.3447\n",
      "Epoch [7/10], Step [440/753], Train Loss: 0.3687, Validation Loss: 0.3346\n",
      "Epoch [7/10], Step [460/753], Train Loss: 0.3624, Validation Loss: 0.3307\n",
      "Epoch [7/10], Step [480/753], Train Loss: 0.3518, Validation Loss: 0.3424\n",
      "Epoch [7/10], Step [500/753], Train Loss: 0.3540, Validation Loss: 0.3257\n",
      "Epoch [7/10], Step [520/753], Train Loss: 0.3527, Validation Loss: 0.3505\n",
      "Epoch [7/10], Step [540/753], Train Loss: 0.3530, Validation Loss: 0.3922\n",
      "Epoch [7/10], Step [560/753], Train Loss: 0.3359, Validation Loss: 0.4145\n",
      "Epoch [7/10], Step [580/753], Train Loss: 0.3514, Validation Loss: 0.3379\n",
      "Epoch [7/10], Step [600/753], Train Loss: 0.3506, Validation Loss: 0.3437\n",
      "Epoch [7/10], Step [620/753], Train Loss: 0.3883, Validation Loss: 0.3347\n",
      "Epoch [7/10], Step [640/753], Train Loss: 0.3555, Validation Loss: 0.3500\n",
      "Epoch [7/10], Step [660/753], Train Loss: 0.3617, Validation Loss: 0.3508\n",
      "Epoch [7/10], Step [680/753], Train Loss: 0.3656, Validation Loss: 0.3562\n",
      "Epoch [7/10], Step [700/753], Train Loss: 0.3385, Validation Loss: 0.3807\n",
      "Epoch [7/10], Step [720/753], Train Loss: 0.3637, Validation Loss: 0.3460\n",
      "Epoch [7/10], Step [740/753], Train Loss: 0.3667, Validation Loss: 0.3679\n",
      "Epoch [8/10], Step [20/753], Train Loss: 0.5940, Validation Loss: 0.3436\n",
      "Epoch [8/10], Step [40/753], Train Loss: 0.3523, Validation Loss: 0.3446\n",
      "Epoch [8/10], Step [60/753], Train Loss: 0.3404, Validation Loss: 0.3475\n",
      "Epoch [8/10], Step [80/753], Train Loss: 0.3760, Validation Loss: 0.3492\n",
      "Epoch [8/10], Step [100/753], Train Loss: 0.3559, Validation Loss: 0.3330\n",
      "Epoch [8/10], Step [120/753], Train Loss: 0.3856, Validation Loss: 0.3622\n",
      "Epoch [8/10], Step [140/753], Train Loss: 0.3504, Validation Loss: 0.3422\n",
      "Epoch [8/10], Step [160/753], Train Loss: 0.3742, Validation Loss: 0.3492\n",
      "Epoch [8/10], Step [180/753], Train Loss: 0.3704, Validation Loss: 0.3610\n",
      "Epoch [8/10], Step [200/753], Train Loss: 0.3498, Validation Loss: 0.3777\n",
      "Epoch [8/10], Step [220/753], Train Loss: 0.2892, Validation Loss: 0.3401\n",
      "Epoch [8/10], Step [240/753], Train Loss: 0.3650, Validation Loss: 0.3658\n",
      "Epoch [8/10], Step [260/753], Train Loss: 0.3774, Validation Loss: 0.4057\n",
      "Epoch [8/10], Step [280/753], Train Loss: 0.3614, Validation Loss: 0.3186\n",
      "Epoch [8/10], Step [300/753], Train Loss: 0.3250, Validation Loss: 0.3532\n",
      "Epoch [8/10], Step [320/753], Train Loss: 0.3363, Validation Loss: 0.3598\n",
      "Epoch [8/10], Step [340/753], Train Loss: 0.3760, Validation Loss: 0.3029\n",
      "Epoch [8/10], Step [360/753], Train Loss: 0.3771, Validation Loss: 0.3721\n",
      "Epoch [8/10], Step [380/753], Train Loss: 0.3398, Validation Loss: 0.3531\n",
      "Epoch [8/10], Step [400/753], Train Loss: 0.3547, Validation Loss: 0.3359\n",
      "Epoch [8/10], Step [420/753], Train Loss: 0.3825, Validation Loss: 0.3224\n",
      "Epoch [8/10], Step [440/753], Train Loss: 0.3445, Validation Loss: 0.3678\n",
      "Epoch [8/10], Step [460/753], Train Loss: 0.3786, Validation Loss: 0.3400\n",
      "Epoch [8/10], Step [480/753], Train Loss: 0.3216, Validation Loss: 0.3452\n",
      "Epoch [8/10], Step [500/753], Train Loss: 0.3584, Validation Loss: 0.3201\n",
      "Epoch [8/10], Step [520/753], Train Loss: 0.3539, Validation Loss: 0.3404\n",
      "Epoch [8/10], Step [540/753], Train Loss: 0.3467, Validation Loss: 0.3186\n",
      "Epoch [8/10], Step [560/753], Train Loss: 0.3731, Validation Loss: 0.3334\n",
      "Epoch [8/10], Step [580/753], Train Loss: 0.3680, Validation Loss: 0.3527\n",
      "Epoch [8/10], Step [600/753], Train Loss: 0.3611, Validation Loss: 0.3337\n",
      "Epoch [8/10], Step [620/753], Train Loss: 0.3548, Validation Loss: 0.3536\n",
      "Epoch [8/10], Step [640/753], Train Loss: 0.3449, Validation Loss: 0.3310\n",
      "Epoch [8/10], Step [660/753], Train Loss: 0.3609, Validation Loss: 0.3324\n",
      "Epoch [8/10], Step [680/753], Train Loss: 0.3375, Validation Loss: 0.3285\n",
      "Epoch [8/10], Step [700/753], Train Loss: 0.3309, Validation Loss: 0.3758\n",
      "Epoch [8/10], Step [720/753], Train Loss: 0.4024, Validation Loss: 0.3732\n",
      "Epoch [8/10], Step [740/753], Train Loss: 0.3619, Validation Loss: 0.3529\n",
      "Epoch [9/10], Step [20/753], Train Loss: 0.5515, Validation Loss: 0.3382\n",
      "Epoch [9/10], Step [40/753], Train Loss: 0.3821, Validation Loss: 0.3688\n",
      "Epoch [9/10], Step [60/753], Train Loss: 0.3289, Validation Loss: 0.3140\n",
      "Epoch [9/10], Step [80/753], Train Loss: 0.3813, Validation Loss: 0.3502\n",
      "Epoch [9/10], Step [100/753], Train Loss: 0.3782, Validation Loss: 0.3410\n",
      "Epoch [9/10], Step [120/753], Train Loss: 0.3637, Validation Loss: 0.3314\n",
      "Epoch [9/10], Step [140/753], Train Loss: 0.3652, Validation Loss: 0.3568\n",
      "Epoch [9/10], Step [160/753], Train Loss: 0.3805, Validation Loss: 0.3362\n",
      "Epoch [9/10], Step [180/753], Train Loss: 0.3763, Validation Loss: 0.3329\n",
      "Epoch [9/10], Step [200/753], Train Loss: 0.3474, Validation Loss: 0.3440\n",
      "Epoch [9/10], Step [220/753], Train Loss: 0.3582, Validation Loss: 0.4026\n",
      "Epoch [9/10], Step [240/753], Train Loss: 0.3597, Validation Loss: 0.3225\n",
      "Epoch [9/10], Step [260/753], Train Loss: 0.3563, Validation Loss: 0.3529\n",
      "Epoch [9/10], Step [280/753], Train Loss: 0.3621, Validation Loss: 0.3286\n",
      "Epoch [9/10], Step [300/753], Train Loss: 0.3336, Validation Loss: 0.3749\n",
      "Epoch [9/10], Step [320/753], Train Loss: 0.3855, Validation Loss: 0.3341\n",
      "Epoch [9/10], Step [340/753], Train Loss: 0.3219, Validation Loss: 0.3240\n",
      "Epoch [9/10], Step [360/753], Train Loss: 0.3197, Validation Loss: 0.3396\n",
      "Epoch [9/10], Step [380/753], Train Loss: 0.3478, Validation Loss: 0.3230\n",
      "Epoch [9/10], Step [400/753], Train Loss: 0.3540, Validation Loss: 0.3814\n",
      "Epoch [9/10], Step [420/753], Train Loss: 0.3265, Validation Loss: 0.3983\n",
      "Epoch [9/10], Step [440/753], Train Loss: 0.3900, Validation Loss: 0.3002\n",
      "Epoch [9/10], Step [460/753], Train Loss: 0.3333, Validation Loss: 0.3381\n",
      "Epoch [9/10], Step [480/753], Train Loss: 0.3582, Validation Loss: 0.3227\n",
      "Epoch [9/10], Step [500/753], Train Loss: 0.3211, Validation Loss: 0.3960\n",
      "Epoch [9/10], Step [520/753], Train Loss: 0.3355, Validation Loss: 0.3479\n",
      "Epoch [9/10], Step [540/753], Train Loss: 0.3605, Validation Loss: 0.3918\n",
      "Epoch [9/10], Step [560/753], Train Loss: 0.3590, Validation Loss: 0.3535\n",
      "Epoch [9/10], Step [580/753], Train Loss: 0.2845, Validation Loss: 0.3442\n",
      "Epoch [9/10], Step [600/753], Train Loss: 0.3703, Validation Loss: 0.3530\n",
      "Epoch [9/10], Step [620/753], Train Loss: 0.3390, Validation Loss: 0.3432\n",
      "Epoch [9/10], Step [640/753], Train Loss: 0.3557, Validation Loss: 0.3390\n",
      "Epoch [9/10], Step [660/753], Train Loss: 0.3388, Validation Loss: 0.3514\n",
      "Epoch [9/10], Step [680/753], Train Loss: 0.3493, Validation Loss: 0.3421\n",
      "Epoch [9/10], Step [700/753], Train Loss: 0.3499, Validation Loss: 0.3250\n",
      "Epoch [9/10], Step [720/753], Train Loss: 0.3601, Validation Loss: 0.3810\n",
      "Epoch [9/10], Step [740/753], Train Loss: 0.3650, Validation Loss: 0.3410\n",
      "Epoch [10/10], Step [20/753], Train Loss: 0.6066, Validation Loss: 0.3431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [40/753], Train Loss: 0.3393, Validation Loss: 0.3363\n",
      "Epoch [10/10], Step [60/753], Train Loss: 0.3548, Validation Loss: 0.3524\n",
      "Epoch [10/10], Step [80/753], Train Loss: 0.3517, Validation Loss: 0.3735\n",
      "Epoch [10/10], Step [100/753], Train Loss: 0.3986, Validation Loss: 0.3493\n",
      "Epoch [10/10], Step [120/753], Train Loss: 0.3484, Validation Loss: 0.3498\n",
      "Epoch [10/10], Step [140/753], Train Loss: 0.3646, Validation Loss: 0.3387\n",
      "Epoch [10/10], Step [160/753], Train Loss: 0.3165, Validation Loss: 0.3195\n",
      "Epoch [10/10], Step [180/753], Train Loss: 0.3452, Validation Loss: 0.3568\n",
      "Epoch [10/10], Step [200/753], Train Loss: 0.3750, Validation Loss: 0.3369\n",
      "Epoch [10/10], Step [220/753], Train Loss: 0.3610, Validation Loss: 0.3179\n",
      "Epoch [10/10], Step [240/753], Train Loss: 0.3334, Validation Loss: 0.3410\n",
      "Epoch [10/10], Step [260/753], Train Loss: 0.3202, Validation Loss: 0.3725\n",
      "Epoch [10/10], Step [280/753], Train Loss: 0.3403, Validation Loss: 0.3519\n",
      "Epoch [10/10], Step [300/753], Train Loss: 0.3416, Validation Loss: 0.3358\n",
      "Epoch [10/10], Step [320/753], Train Loss: 0.3620, Validation Loss: 0.3134\n",
      "Epoch [10/10], Step [340/753], Train Loss: 0.3643, Validation Loss: 0.4002\n",
      "Epoch [10/10], Step [360/753], Train Loss: 0.3425, Validation Loss: 0.3351\n",
      "Epoch [10/10], Step [380/753], Train Loss: 0.3465, Validation Loss: 0.3217\n",
      "Epoch [10/10], Step [400/753], Train Loss: 0.4143, Validation Loss: 0.3628\n",
      "Epoch [10/10], Step [420/753], Train Loss: 0.3170, Validation Loss: 0.3526\n",
      "Epoch [10/10], Step [440/753], Train Loss: 0.2879, Validation Loss: 0.3455\n",
      "Epoch [10/10], Step [460/753], Train Loss: 0.3311, Validation Loss: 0.3370\n",
      "Epoch [10/10], Step [480/753], Train Loss: 0.3640, Validation Loss: 0.3698\n",
      "Epoch [10/10], Step [500/753], Train Loss: 0.3464, Validation Loss: 0.3334\n",
      "Epoch [10/10], Step [520/753], Train Loss: 0.3515, Validation Loss: 0.3504\n",
      "Epoch [10/10], Step [540/753], Train Loss: 0.3755, Validation Loss: 0.3583\n",
      "Epoch [10/10], Step [560/753], Train Loss: 0.3760, Validation Loss: 0.3447\n",
      "Epoch [10/10], Step [580/753], Train Loss: 0.3147, Validation Loss: 0.3597\n",
      "Epoch [10/10], Step [600/753], Train Loss: 0.3733, Validation Loss: 0.3126\n",
      "Epoch [10/10], Step [620/753], Train Loss: 0.3888, Validation Loss: 0.3530\n",
      "Epoch [10/10], Step [640/753], Train Loss: 0.3779, Validation Loss: 0.3250\n",
      "Epoch [10/10], Step [660/753], Train Loss: 0.3233, Validation Loss: 0.3906\n",
      "Epoch [10/10], Step [680/753], Train Loss: 0.3136, Validation Loss: 0.3313\n",
      "Epoch [10/10], Step [700/753], Train Loss: 0.3209, Validation Loss: 0.3582\n",
      "Epoch [10/10], Step [720/753], Train Loss: 0.3255, Validation Loss: 0.3178\n",
      "Epoch [10/10], Step [740/753], Train Loss: 0.3616, Validation Loss: 0.3390\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 10 # how many times you want to train on the dataset\n",
    "model.train() # set the model to training mode\n",
    "\n",
    "# these variables are used to store the losses\n",
    "running_loss = 0\n",
    "training_loss = []\n",
    "validating_loss = []\n",
    "\n",
    "\n",
    "# loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    batch_size = train_loader_X.batch_size\n",
    "    # loop over the batches\n",
    "    for i, batch in enumerate(train_loader_X):\n",
    "        model.train()\n",
    "        batch = torch.tensor(batch)\n",
    "        xs = batch[:, :-1]\n",
    "#        print(xs.shape)\n",
    "        ys = batch[:,-1].clamp(max=1).long()\n",
    "#         print(xs)\n",
    "#         ys = F.softmax(batch[:,-1], dim=1).long()\n",
    "#        print(ys)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad() # reset the gradients\n",
    "        pred_ys = model(xs) # generate the predictions\n",
    "#         print(pred_ys)\n",
    "        loss = criterium(pred_ys, ys) # compute the loss\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # optmizes here\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 20 == 0: # every 100 batches print statistics about the training\n",
    "            running_loss /= 20 # training loss on the last batch\n",
    "            training_loss.append(running_loss) # keep track of the training loss\n",
    "            model.eval() # set the model to eval, we can now test the model\n",
    "            validation_loss = compute_loss_validation(model) # compute the validation loss\n",
    "            validating_loss.append(validation_loss) # keep track of the validation loss\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f, Validation Loss: %.4f'%(\n",
    "              epoch+1,\n",
    "              epochs,\n",
    "              i+1,\n",
    "              len(train_dataset_X)//batch_size, running_loss, validation_loss))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2d37069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABrF0lEQVR4nO2dd5xcVd24n3Onbt/N7qZ3SICEVEIA6VIEaVIUBAsWUOyoCOqr4k99RURfBFFeVMCCIi9FUHoJvSVACAkQUkhvm91snz7n98ftU7ZkZ+Zuds7z+SRz57Zz5u7M+Z5vPUJKiUKhUCjKF83rDigUCoXCW5QgUCgUijJHCQKFQqEoc5QgUCgUijJHCQKFQqEoc/xed2CwNDU1yalTpw75Pq2trQA0NjaWdFuhUCi84LXXXtstpWzOdWyfEwRTp05l2bJlQ77P7bffDsDFF19c0m2FQqHwAiHExnzHlGlIoVAoyhwlCBQKhaLMUYJAoVAoypx9zkegUChKTyKRYMuWLUSjUa+7ouiHcDjMxIkTCQQCA75GCQKFQtEvW7ZsoaamhqlTpyKE8Lo7ijxIKWltbWXLli1MmzZtwNcp05BCoeiXaDRKY2OjEgLDHCEEjY2Ng9bclCBQKBQDQgmBfYO9+Tsp01AJ6IgkCPmVzFUoFMMTNTqVgPUt3WzvUE42hWJvaW9v53e/+91eXfvhD3+Y9vb2AZ9/9dVXc9111+1VW/sqShCUgLTUnTgKhWLv6EsQpFKpPq996KGHqK+vL0KvRg5KECgUimHPVVddxbp165g/fz5XXHEFTz/9NMcffzwXXnghc+bMAeAjH/kIhxxyCLNnz+aWW26xrp06dSq7d+9mw4YNHHTQQVxyySXMnj2bk08+mUgk0me7y5cv5/DDD2fu3LmcffbZ7NmzB4AbbriBWbNmMXfuXC644AIAnnnmGebPn8/8+fNZsGABXV1dRXoahUf5CEqE0gcUI4Uf/3sVb2/rLOg9Z42v5UdnzM57/JprrmHlypUsX74cgKeffppXX32VlStXWmGSt956K6NGjSISiXDooYdy7rnnZhV6XLNmDf/4xz/4wx/+wMc+9jHuuecePvGJT+Rt91Of+hQ33ngjxx57LD/84Q/58Y9/zPXXX88111zD+++/TygUssxO1113HTfddBNHHnkk3d3dhMPhoT2UEqI0AoVCsU+yePFiV6z8DTfcwLx58zj88MPZvHkza9asybpm2rRpzJ8/H4BDDjmEDRs25L1/R0cH7e3tHHvssQB8+tOf5tlnnwVg7ty5XHTRRfztb3/D79fn00ceeSTf/OY3ueGGG2hvb7f27wvsOz3dh5HWfwrFvk9fM/dSUlVVZW0//fTTPPHEE7z00ktUVlZy3HHH5YylD4VC1rbP5+vXNJSPBx98kGeffZYHHniAn/zkJ6xatYqrrrqK0047jYceeojDDz+cJ554ggMPPHCv7l9qlEZQCqRUckChGAI1NTV92tw7OjpoaGigsrKSd999l5dffnnIbdbV1dHQ0MBzzz0HwF//+leOPfZY0uk0mzdv5vjjj+faa6+lvb2d7u5u1q1bx5w5c7jyyitZtGgR77777pD7UCqURqBQKIY9jY2NHHnkkRx88MGceuqpnHbaaa7jp5xyCjfffDNz587lgAMO4PDDDy9Iu3/+85/54he/SG9vL9OnT+e2224jlUrxiU98go6ODqSUXH755dTX1/ODH/yAJUuW4PP5mDVrFqeeempB+lAKlCAoARJlGVIohsrf//531/vjjjvO2g6FQjz88MM5rzP9AE1NTaxcudLa/+1vfzvn+VdffbW1PX/+/JzaxfPPP5+178Ybb8zX9WGPMg0pFApFmaMEQalQCWUKhWKYogRBCVAyQKFQDGeUICgRShYoFIrhihIECoVCUeYoQVACJMo8pFAohi9KEJQEJQUUilJTXV0NwLZt2zjvvPNynnPcccexbNmyPu9z/fXX09vba70fbFnrfAynctdKECgUihHN+PHjufvuu/f6+kxBMBLLWhdVEAghThFCrBZCrBVCXJXj+BVCiOXGv5VCiJQQYlQx++QFyiykUAyNK6+80rUewdVXX82vfvUruru7OeGEE1i4cCFz5szh/vvvz7p2w4YNHHzwwQBEIhEuuOAC5s6dy/nnn++qNXTZZZexaNEiZs+ezY9+9CNAL2S3bds2jj/+eI4//njALmsN8Otf/5qDDz6Ygw8+mOuvv95qb18rd120zGIhhA+4CTgJ2AIsFUI8IKV82zxHSvlL4JfG+WcAl0sp24rVJy8wF6TxQhbs6Ijy3JoWPrpokgetK0YsD18FO94q7D3HzoFTr8l7+IILLuAb3/gGX/rSlwC46667eOSRRwiHw9x3333U1taye/duDj/8cM4888y86/b+/ve/p7KykhUrVrBixQoWLlxoHfvZz37GqFGjSKVSnHDCCaxYsYKvfe1r/PrXv2bJkiU0NTW57vXaa69x22238corryCl5LDDDuPYY4+loaFhnyt3XUyNYDGwVkq5XkoZB+4Ezurj/I8D/yhifzwhbUoAD9SC+5dv5Yq7VxBN9L2Ck0Ix3FmwYAG7du1i27ZtvPnmmzQ0NDB58mSklHzve99j7ty5nHjiiWzdupWdO3fmvc+zzz5rDchz585l7ty51rG77rqLhQsXsmDBAlatWsXbb7+d7zaAXmbi7LPPpqqqiurqas455xyrQN2+Vu66mLWGJgCbHe+3AIflOlEIUQmcAnwlz/FLgUsBJk+eXNheFhkvl6hMGlIolVa2KUUB6WPmXkzOO+887r77bnbs2GGZSe644w5aWlp47bXXCAQCTJ06NWf5aSe5tIX333+f6667jqVLl9LQ0MDFF1/c7336+m3va+Wui6kR5NLN8j25M4AX8pmFpJS3SCkXSSkXNTc3F6yDpcAcg70Yis0valo5KRQjgAsuuIA777yTu+++24oC6ujoYPTo0QQCAZYsWcLGjRv7vMcxxxzDHXfcAcDKlStZsWIFAJ2dnVRVVVFXV8fOnTtdBezylcA+5phj+Ne//kVvby89PT3cd999HH300YP+XMOh3HUxNYItgNM4PRHYlufcCxiBZiEAL1cikB4KIYWi0MyePZuuri4mTJjAuHHjALjooos444wzWLRoEfPnz+93ZnzZZZfxmc98hrlz5zJ//nwWL14MwLx581iwYAGzZ89m+vTpHHnkkdY1l156Kaeeeirjxo1jyZIl1v6FCxdy8cUXW/f4/Oc/z4IFC/o0A+XD63LXxRQES4EZQohpwFb0wf7CzJOEEHXAsUB+T8o+jJeDsaWNpD1oXKEoAm+95XZSNzU18dJLL+U8t7u7G9CjfMzy0xUVFdx55505z7/99ttz7v/qV7/KV7/6Veu9c6D/5je/yTe/+U3X+c72YN8od100QSClTAohvgI8CviAW6WUq4QQXzSO32ycejbwmJSyp1h98RLLKuOBJEgr05BCoRgARV2YRkr5EPBQxr6bM97fDtxezH54iZeDsPIRKBSKgaAyi4uMl0Owh8qIYgTiZQScYuDszd9JCYIik/YwoUyZhhSFIhwO09raqoTBMEdKSWtr66CTzNSaxUXG/t2U/gdkOYvVb1cxRCZOnMiWLVtoaWnxuiuKfgiHw0ycOHFQ1yhBUGSsEhMeDMZSCQJFgQgEAkybNs3rbiiKhDINFRkvB2HlLFYoFANBCYIi4+UgrHwECoViIChBUGS8jNxRPgKFQjEQlCAoMt7mEbhfFQqFIhdKEBQblVmsUCiGOUoQFBlVfVShUAx3lCAoMp5WHzVe1XIECoWiL5QgGChbX4ONLw76srSnCWWqyIRCoegflVA2UBJ7t8KQl2sWm0JIaQQKhaIvlEZQZLyclCsfQXlw//Kt/P2VTV53Q7EPowRBkfE2s9j7PiiKz92vbeGfS5UgUOw9ShAUGVV9VFEK1F9YMRSUICgyXv5AVWZxeZCWUgl7xZBQgqDIqFpDimIjpRL2iqGhBEGR8dROrzSCskDXCLzuhWJfpnwFQWQPpJNFb8bLFZ2URlAepKVaRlIxNMozj6C3DXaugor6ojclc2yVCpVHUCYo05BiiJSnRhDr1F8T0aI35WXUkJdCSFE6lLNYMVTKUxCYAkAU/+N7mVBmm4ZK37aidEiU+U8xNMpTEMS79Vchit6Ut+sRGIJASYIRTVp6WdpQMRIoT0EQ69JfS6gReJJQljZe1SgxokkrH4FiiChBUGQ8LTGBdL0qRijKR6AYIuUpCCzTUAkEgYeDsMosLi1tPXFPwjiVRqAYKuUpCEqoEagVysqDlq4Yh/33E7ywtrXkbUuURqAYGkoQFBnpYWqx2aTyERSfjkiCREqyuztW8rbTaaURKIZGeQuCEuDlIGzlMKhRouh4qX2lpVR/Y8WQKE9BYPoI9sZgM+gfnPcrlJXTGPF/yzbz1X+8UfJ2vc7iVlqfYiiUjSB46t2dHHPtEjbs7rE1gr0ZIWV6UKcPB42gnOzHb2xu54W1u0vebirtnfal5xGUz99YUXjKRhAkUpJNbb10x5JDEwTp1KBOHw5jsBd9+O1Ta1i7q3QmOBPpUSilbYYredNIqTSCUiGlpCua8LobBaeogkAIcYoQYrUQYq0Q4qo85xwnhFguhFglhHimWH0JB3wAxJIph49gcLN7AOTgBIGXA4RXGkEsmeK6x97jkZU7Stou6DNzLzKpbce88hGMZB57eyeH/feT+oRyBFG06qNCCB9wE3ASsAVYKoR4QEr5tuOceuB3wClSyk1CiNHF6k/Yr8u8aCJtCIKxI14j8Cqz2GzXG+Hn7bKg3oQJDw/NsxzY1RmlN56iN5akOjRyijcXUyNYDKyVUq6XUsaBO4GzMs65ELhXSrkJQEq5q1idqQjqGkEknrKdxXvlIxisIPB+PYJS98HLYnf67Nibdp2vpUQVnSsdXgcFFItiCoIJwGbH+y3GPiczgQYhxNNCiNeEEJ/KdSMhxKVCiGVCiGUtLS171RnTNBRNpiDSbuwtvrPYw+KjnrXt6aAovTLPuF9L27ZaoaxUjNQAjGIKglylPTOfnh84BDgN+BDwAyHEzKyLpLxFSrlISrmoubl5rzoT9huCIJ7SVyeDQQ/qgG33GOjpHtah9iq23Q5b9cZe7o0A8jZxUPkISoMZHaYEwcDZAkxyvJ8IbMtxziNSyh4p5W7gWWBeMToTDuofNRnpsM07JTENDb6JQuHVLFV6aBpKpb2ZHXutEXjxPWvtjvHlO14fkVE0+fB0DfIiUkxBsBSYIYSYJoQIAhcAD2Sccz9wtBDCL4SoBA4D3ilGZ8IBHzPEFvZ//++OvcV3FnsbVuiVj8B8LZ/ZcTmaw1Zs7eDBt7bz3s7Shwl7xUg1DRXN7S2lTAohvgI8CviAW6WUq4QQXzSO3yylfEcI8QiwAj2W849SypXF6E/Y7+N030ssWn+fvkPzlyShzNmClBJRgsVwTLzKLPbaWexVu+CdwPcmWsm7v7NXjNRs/aLGP0kpHwIeyth3c8b7XwK/LGY/AAI+wXYc/gVfYO98BEOIGkpL8JVODnjoI/Bu1uSdj8Buv9SkvXKQm+HJZSQJRqpGUDaZxUIIdmmONAVfcC+dxXvvI/DKaVt6H4HRrgcDRMqjSpyeagR4qwWVkRwYsVpQ2QgCgFb/GPuNL1jyWkOlHiSsFcrKKI/Aa7+IF0aatMST+GQvo8O8YqR+5rISBB0BpyAIUApnsds0VOLByaMMXy+dxV4JIa+FXzk9ay8ZqZ+5rASBLxiy3whNVR8tVrueVuI0X0vbtpfrEUjpbVmNkWYv7wsvJznFpKwEgZldDIAQ9PnzefyH0JmZ9sCgncXONko/OJntlrRZz6OGnK8la9fT+kpeaQR2++WCbXr0uCMFpqwEQUXAx3+qz4WDzwWEEYCdZ2B/7zGItmfvH3Qegb3tmY+gxPPF4WAa8ipk1gstSOJ1nkrp2/aKkaoFlZUgCAd83Fr5OTjvVkMjAFJ5siLTydxphEPILC6XqCFPNQKr4mp5PGvwzhQ3UgfFvhipeQRlJgg0vQw1YH30dB+CwJxJO/0Ce11ryLvib6UeILyK3AHvhJCXs2PPTIAelTn3kpEq/MpMEPiIJowZfb8aQcqhETgG/6FkFu9F2sKQ8Gj24nWJCf21fMxhZoulbjs1QgfFvvAycbCYlK0giCWNP2Q+QSBT5NQIhpRZXB6q+/BwFnvTrpdaUMl9UB5+Zq8wzXAjTQsqM0GgEU2m6Ykl2doR1Xf2ZRrKpREMIbO49KYh92vJ2vXITg/2LNUre7kX44NXs9SRukhLX6iEshFAyO+jrSfO31/ZhGQwzuK91wjSw0Aj8GxQ9EQjcL+WCm9rDXkbKTXSzCR94aXALyZlJQjG1OoJZT976B1LEOzp7s19cl5n8RA0Ao8GJ6/a9XKBmHIyh1l/Z8/ChEvarKdY368R9qHLShB86oipHDtTr0BqCoIlb+dIGoOCOYtdUUOlHJwSES5M3IuPlIdOxJI2a7TplSDQXz2JGsIjv4iHGeReMVKFX1kJgnDAx7yJdYAtCP79+obc86h8GsFQooYGdeUQee7XfDn1Vz7qe8bDPAIPfASe1Vfy0lns7kPp2vXu73zPa1t4Ye3ukrfr5d+5mJSVIACorwwCYA7np0fuJ5FMw7blcNen7RPzJZTtK0XnYvqqUZXEPKu740mWrccRNF6awzwLEy51WDRw05K1/P2VTSVvV2kEI4RRVbogMDWCc33PIRMRiHfD2/+C3lb9xHShwkdzb5cK4VVZZLwZIMrRNORVJIuXwi8lpbWQfCnx8jMXk7ITBPWVAWPLsVRYMmZvp+LGhixI+Kjzu9rvl2ftk9DbNqj758VaElOW3LFlx1qXo5mkpM26Bn/vzGGlbddsO+XJ90sJghGBqRH4nZ88GbG3pXOJq0L4CPr4oUoJezbo2/Fe+Ns5cMdHB3X//NiCrvQ+Am/a1dv02ExS8lm5sw9eRQ15MCCnvYncsRcgGlmUnSBoMHwEvVo1iVADACIZtU+QkqzaDEPKI8i4tZMNz8Nv5uvCwExs2/3eoO7fH3qx7fLxEXiljXjlPHTnqZS27ZT1rEvbrt6mtxqBchbv4zQYGoFP00hUTwAyBUE/GsFgDd99OYt7d+ttRPY4TE4FWt3eMA0JD9az9bYMtfu1ZO16JYBc294IPy8G5FTaGx+B9Xf2wP9VTPxed6DUVAV9BHwCvyZA0+WgliqRRpB10LhXKmlvF0gOmAhkWa1Z7LWz2CvfBHhnDvOqvlI5+aCKTdkJAiEEDZVBfD4Noemjri8V1dcw1qoK7yPoSyNIJ+1Xq+ZRgSVBrnaLjJcONenR4ORd4bfsPpQK6+/ska3eE43Aw0lOMSk70xDAD06fxbi6MEIz5KCU4AuCP4g++GdoBM5IoXSKSGLgWkGfPgLzvumkXfNIFF4QlEtpC/B+QPaqxlHmdinwMigglS69yRO8m2gUm7IUBGfMG091yI+mOdYw9gV0YeDSCNC3He/f3rqHNze309oTZyC4bLh5NYKEvS0K9CexfAReOhG9sR3rbZe2Xa8L/GVulwIvY+rTsvRh0Wa7UKZRQ0KIrwshaoXOn4QQrwshTi5254qN0DTS0piBawHwhQzTj+PPvPEFWP2g9Xbbnh4AIvGBaQWuOO/Mg5YgSNnbBTMN2fcppyUMvbPV668l10Sc2x4JfE80v7TKIygkA51+flZK2QmcDDQDnwGuKVqvSoQmBL3oFUnxBXTTUIYGAMDrf7U20yl9wB6oBadPG67pb3CZhgqrpOlRQ145TkvaLODdLNXrRYCgfHInQI9U8jKPoFx9BOaw92HgNinlmxTDq1liNCGIm/5yzZ9bIwBwhJemjbgxbYCSoM8fqqkFpBK2s7hQPgKHaaj09nJvzCTgpY/AG5OUK7LZM9NQSZu12vRCIxipq7INVBC8JoR4DF0QPCqEqAH2+UhaISCIPhhv70rRGjW0gUw5ELPXLBi0RuDYzo4acjiLC51HULaZxearR6ahUsfyO9orFy0IDNOQJ7WszNeRJQgGGj76OWA+sF5K2SuEGIVuHtrnCaHPxHuS8H57EiqzNYJopMcaV9PGLL6gGsGIMw15O0DobZe2Xa/s5X3mqZSobc/yCLwMH93np8FuBjrqHAGsllK2CyE+AfwX0FG8bpWOgNBn4gl8xKUfSNPSFXWdE8aOEEqn9PMHOm/vM7wvVx5BgU1DXsQ3DIeEsnKpxOkuOueVwC9ps0gpPTMNjVSNYKCC4PdArxBiHvAdYCPwl6L1ygOS+IgTQKbT7OqMuI75hS3+ZSHXI8jlLC6Qacgssy1ytVtkvIy19qzEhEdRQ33mqRS7bY/ChO0y5x76CErecnEZqCBISv0JnAX8Rkr5G6CmeN0qPWnhJ4HfcAbn/zObzuKBfvddGkHWzZzOYrPERIEEgdFY2ZmGPI7n91IjKBdfkPmMVdG5wjFQH0GXEOK7wCeBo4UQPiDQzzX7FOFQiFivHynTfc7JTR/BQJ2Cfa5H4MojKE5msY+0dwOEpwvTlLpd/dXLPAKvBH6pB0XTH+NN0TnjdWTJgQFrBOcDMfR8gh3ABOCX/V0khDhFCLFaCLFWCHFVjuPHCSE6hBDLjX8/HFTvC0goECBOAKTsc1UvafgIBqwR9LUegakFbHgWVt6rbxfIWSwNs5NPpDxbsKScEspsk4E3g7Heh5I27eGzdr+WkpGaUDYgjUBKuUMIcQdwqBDidOBVKWWfPgJDa7gJOAnYAiwVQjwgpXw749TnpJSn70XfC8K3qq+htmsLQb9GUgQQ/WgEExLvozF9wD93tw03j0bwzr8dOwtkGjKEjJ906c0kXmacep1QVmItqM+ExaK37VGElmka8sRHoL+WpUYghPgY8CrwUeBjwCtCiPP6uWwxsFZKuV5KGQfuRPcxDCteSR3AdjkKv09DanrRub40giNiLzJV7Bj44NpXiYlclUwL5SMwBIFG2sPM4tIPTF7NFsuzDLU3QtcyDSkfQcEYqB3i+8ChUspPSyk/hT7I/6CfayYAmx3vtxj7MjlCCPGmEOJhIcTsXDcSQlwqhFgmhFjW0tIywC4PjJ6YPisP+ATCrxed628orhER96CeTsJjP8j5S3T5CDKnEVZ9IScF8hFIUyNIeebM2yeWbdz+JiSi/Z/XD14VI3MHI5R6QM7uQymwNBFP8wjKUxBoUspdjvetA7g214iW+fReB6ZIKecBNwL/ynUjKeUtUspFUspFzc3NA+zywOiJGQOmpoEvhOYsQ52HFJr7y79nI7x4A/RkCymXjyDzYC5BUCgfgaER+Eh7WGKitO2mBjs77toJ/3sM/PvrQ27bq5BZt/AradOeDcjeagT668gSAwMXBI8IIR4VQlwshLgYeBB4qJ9rtgCTHO8nAtucJ0gpO6WU3cb2Q0BACNE0wD4VhLgxrQn4BFoghOjHNASmIHBOxfLnFvQdNZTjukJFDaVtjaD0oZTmq3dmkgG1HevUX7csLVjbXmlfmdulbLu8fATefOZiM1Bn8RVCiHOBI9Fn+rdIKe/r57KlwAwhxDRgK3ABcKHzBCHEWGCnlFIKIRajC6bWQX6GghDwaWiBEBA3tIL8pDLPMG39OQbxvjOLcwmQwvoIfJ74CLz5sfT5rHNeYP7dhq6FeZbR7NwuE7+I5bD1sProSPMRDHipSinlPcA9gzg/KYT4CvAo4ANulVKuEkJ80Th+M3AecJkQIglEgAukR0/Ypwl8fmNh+37q6UmE60cXT+ZPBnOXAMg4WETTEE7TUGHuOPCm9xWNwMq6G7rw9Tq5CrxLovMsj8BDZ3FZhY8KIbrIbQ7TqxtLWdvX9Ya556GMfTc7tn8L/HbAvS0Cv7toIS898j4AvmAY0CNt+kJDWrb/nliSRMxIBsvx5XDuyS4xkcc0lE5BKg6BioF9iFwYQsYvSu8s9spH4DbDDeCCAmoEw6LWUElb9i6U0svCb2W5HoGUskZKWZvjX01/QmBf4cNzxrFfczUAfkMQ+ETff2WNtPWrS6YdPgWZhq4d8N6j1rlO9XVAzmIEvHgj3Hz0YD5GFlJ6GT7qzaDotBkPrG1TIyiAacijapQuZ7FHTtuSf7+MZ+3legQjTSMoyzWL8xEI6YIgqPX9R3aaW6TE9hjINLSuhb9/zDq3T40gp49AQscW6Nic49ggMH4t5eQsHnQlTqu+U+F8BF7lbGRul6Ztb53F3tayKnnTRUUJAgfCp/sI/APQCMyBJiUdGkGOaaHryzoQZ3E6qZuFkrGhfdsMbcOL8FGv6/0MuO0CrhPtVX0ld3iyN0LIO9+Ed/6JkeYsVoLAQU/KB+Q3Da2Q07kvdSQ+JPFkmhufXENrd8ytEWTQZ5JTLtNQOmWUpJZ5TEcDxBiRPIka8ujHMugsW/P5FsBZ7FmtIcdXzrPELo++X1D6ENKyLjFRLviDunM20IdGEJFBNJGmPZLgV4+/R1tP3HYu5xQEfQxOuZzFMsWerm59OxkbVP/d7ZpRQ15kFrtfS9buYH0EBVwVrpzzCEq9ZKTzGZfaTzBSo4aUIHBw4iK9wkVYyz0TlwgihNEyBnxfX4LAsT1QjeClNTv07VQ8+/hAkc6ic3t/m71heNjLB+IjKFzp7+EQX+5VlJZX5hkovSmuLKOGyo1Aw0QARB8z8V5CWQlnPhyL0GfgnrFlHszlI0gRNNZRTg+lBk7a1gg8cxaX+Ney16ahgvgIvHEiDgeNwMt8Ea80AuUjGMlUjdZnh3lm4hKYOrYJgbSEgebYzhIEbev56fKjMu7gII+zOIh+n1gskn18oHiYWexdHsFgTUOmj6AQeQSDaLeADDqbuoAMB3OYdz4CJQhGLkbhuXxMaqjkjEX766diO2MtMgVBp6u0Ug6NIIdpSKYIGBpGNDIEQWD6CIQXK5R5M1N0O+YHcEF6ZPkIvHJUe5ov4pHWOcLkgBIEWRghpAnpyzokhLCyfZtEJ+GAhk84ZvV5onzM8NIBOYvTKQLC0AiiEXpiSX7+0DtEE/kL2+XEmUcwuCuHjFd21EEnlBUwasirMtTuMuelbtsrzc/e9s5ZXNJmi44SBJn4dY0gYVbfqLSLoeqCoAqAqWIHc+sTbo3AjELJwDQd/e7ptcSS/QiOdIqAYRqKxyK8uK6V/312Pa9v2jO4zyEdtYbK0HY8oBG5gKYh75zF3vkIvDKTuEyAJdcI9FflIxjpGIXnkmi8lp4JzQdYh4QQEKy03k+rlW7HcR6NYILYTZ3oYdW2Tu54eZPj/Nzho7aPIMaeHt1fYa6bMGDSzvBRbwYIb2sNDSZqaN+tNZQepOwrJF5l+Do1v1JrBCO1xMSAq4+WDX69zEStiOjmIYfZQAAkbLt9jT9JRU0AuqFTVkCeYnXjRas9y3cEXadSSTINUDKVtExD8VgvbSlTEAwyucwRPlpyk4FHNWics7QBTRQLmkdg9mHItxpcu85aVmViJukzEq/obXvTbrFRGkEmwSprU9PctmMhBEw+3HpfrSVorNRlaVtGDT6ZJ9cg5LcfeSSaHaaaSiUJSH3QT8SitkYQH5ggeHtbJ6u2dbjDR0u9hOEwmB0PykdQkLa9GRSdzZVLHoFzYuOVs3ikaQRKEGQSsAXBwskNrkNCCKibyObgfgDsF3sbYl0AtFPtOndDS5frvZ8Ul/vvptKZrJZjIPKLNEFDI0jGo7T1DE4j+PANz3HaDc9byW16+OiALi0YXqXhpwY7Oy6os3gQ7RYQL2fHXq3W5WX4qF0+paTNFh0lCDLRbGONP5dGAJYp4Yxtv9ErhQJtssZ1blu3e7ZfSy9f99/L2D2v2ztzZCIDhNAH/0Q8yp5efbs7j4/gW3e9yV1Lc1QqHQbOYijtwDjoQbGApiGvcidcLqoyCQpw+gVK7yMwX0eWJFCCIBf5BgZDEMgcxydMmOg+NY855u0NW7nm4XdJptJ5TRNhI7M4FY9ZGkGvqRHs2QhX18Hb99MTS3LP61v4zj0rsrtqrlksUh4ndg3iQinhga/C1tf7PzfP5bn6kJd0IaOGvDeHlTx01cojKHW73ucRKB9BOTBpMXzjrazdgvyC4IBpU13v/SL3DH791u3c/Mw6fvnYakSeRe8rha5NpJJR9vTqQsH0Ebz+yhL9pBV38ebmdgDCAY0lq3e5qzIaoZGDXZgmnZa8tnGQoaqZ99jbWWq8G17/C6x/ei/bHaQAMqOGClFiwqPkqnJcqtLbPALzdWRJAiUIcqH5oX5y9m7jacnMgUNoEKh07fKTe5CvRo86enlda15BYOLUCLpjes2gPz27zuiMj2WOAfszty3l5fWt1vu0IQj8g/QRvLiulXN//yLv7ugc+EUmPbuBzOidQTRummr20ombGuygmNr3E8qc7ZWNacjDMtRKI1BgzhzTmY9NaFBR79pVF8x9hyqi+Emyq73HMt/kIxmP0hFJ8CFtKVev/SjRaNQucCd8enQQkEjp38ouh0PZFASDLTrXZvgkWroGWQJ740tw3Qxo37T3NfLT+Yv3DYQ+S37nbM9ca3ro8bXe5U4M8jMXtG3j1aOMZi/aVj4CBZoxc0zJjMcmJTQd4No1oTZ3ika1iPB66IvcmfiKSyNInHYD/0ge7zq3N9ILwE8Df6Ix1cLjy96xwlDTwkdXVB8wzVlRPGn/KtIph7PY2GeGovaFWcqiMzLIwbhzqz6gdu/a+4qY5sCcJ0O738v31kfQj0AeWNsehRW6PnNpmx4OGeRqPYLCoATBYBCmRpBpSpAw+kD3rlTugbSGCLWilyliF0La56wfdTSr5FTXuVFDEISMZLRfPLjC8j3c9+YOVm1zm2/iyTQVAT3qSaYdCWVSsqMjyqE/e8JlPsqFKQi6ooMcjM2KrcloxoA8iHtYA/NeCgJXraEBXGD+jQqgEZRjGWqvtCBlGio8ShAMAi2fLVmmoXaCe1+ewaxa2JnJTo1gXVucVMafIxLV1yMw8woqRMzSCJLSR0fE3UY8lbbM3dJZYiKtm3qSacn2Dr39G59cw2OrdmT1LxI3BcEgNYJk1Ho11eYmOvA/f93ARwrLR7B3M/S9LjFRgMSySYn3qaHX06ihUjsohkeVWW+e9whTCJQgGAxmHkFO+2CmkMijEVRhCwLnSmdL1rQjM/4c6YRupzcXqqkgbm1nCg3QNYKEUcJCWCUm9FpDZrE703z055c28uBb27PuEU3oxwetESTj1qv54zzB9zrh534O7RsHdg9TAOy1aWiwzuKhCR4n/91+BZ/xPeLZusHgndO2XMpQy8F+v/YhlCAYBNZsO98J9ZMhZGQYJ3OvJVDj0Aj8whYE967YheYLuM41B32zsF0FMSqMZLNcgiCWTFmOY2FoDprRRizpfu2NJ3MWsouYPoIhaARGF6z6SgMe2Ic4Qx98+KhpGhqiIEglqZK91Imekg2KUkruX77V+nuCF5nF3rTrVULZXodF7wMoQTAIbI0gzwn1k6HZ8BXEukiKQNYpjbjt+rclP8TH498nhY+aSveiOGYVUpNKEaPCzDHI8adzDuyaVWJCItMpl0YgpSSSSNGbo36R5Sw2NIJoIsVV96zoP4rI8hHErB+J1f8+lv50kcNH0BFJMPWqB/n3m9vyXOS43ANn8TvbO3lqpa7xhIiXTCNYsnoXX79zOf/z+HvWvpIvTONRHoFLCyph1JCX5TyKjRIEg8BKKOvnLJOkvyrr6GjhTtZqkfW8lJ5NTcjP54+Z4ToWFAlXkbowccKGRmDmKTTSwX/5/8p+YqurHpHmrIQq08QStkYQTegL2vfGswdA21ms3+vRVTu4c+lmzvrt8/z0P2/n/9jmYJ+KWT9UWyPoP1pJPy97YN7Y2gPAzc+s6/fyQYdSmprKEDSCPzy7nmv/sxyAEImSzRTbevS+b++w17X2LmqotO06CviWWCPwzgxXbJQgGASWaaivL4Ej61iYZiIHtcJtMjJn9kfPbKK5zp2UFiThUv0XaGuYJTYax/RB8/bgtXze/zAf8b3gqlDqXDBHk0mr/HUsmbbOy6URmKahc3f8Bv54krW/snMtHS/dZr2PJlLsePC/4U8n6zss01DMmqUN3jSUfb7poB/IYOP8uyRSab577wo2tfb2394QppWd0QTpuN5GUCRLNijmHIjKJMvWq4Vp9jo/Zh9ArUcwAHY0LII9eQ7WT4bKRvu9QxAEKuugu+97Jw1BUBX06xnNDqqIcWvgWuv9F/wP2vcWSarpZY72PgB19LA66hYELbKWZtHJocnX8XfoRfHiybQVGdSXRnBK7wPQC8kF+jf+idB3AEilr8WnCX54/0quXfkL+0JX+Ki0+ug61h99+AgGYn5wzhQ3tvXy4IrtHDyhjosap+Rpb+g+gq5oknQiAkFdIyhZ6E6OZlT10dK1W2ozXLFRGsEAGHPZA8gJi6z3rjGpboJrDQOnINAq3WWsI6Ii694pY2maqpAfhHuZmoO0jXzQtzxnnwIkLe0AoFF00BtPESTBqJBEI82atF4I7/9Fr+G0JacCukO5tw9BEEm4Z8eZIaqx7ja4uo6x6+5yfIikWyMwnk9wsKYha4ZuCwJTkxnID975QzWL9MUSfcz2CxA11B1LEpL65wuS8FQjKMrMPN5rlVrPxPyblNpH4NUKZYXIaE6m0iXPfRgIShAMABGsQgTC1vugw26fOXgjBJz8M5iw0L1/ypH8ufLirHubpqGasN9VAluiMUa05+1TiKSlDaxLj6NRdNEdS/Jc6Os8JS7DR5r35MSs6+IO01DmGgfbOyJZ2ceZgqBr80oAPtxzv70z2uEIHy2Aj8BhGjIH8oEMcs4ByXScR5N9DPIFyCPwRds4SNOXHy2ljyBXK0Vp+ob58PPs7xHYmkDpk7oc2yXVCJzbe9fu/P/3OMdcu6RAPSocyjS0F+w3uho29XHCB74C793u3icEu7SmrFNdGoFDEMTCTYSju/I2ESDJLG0D7b5GVqcmMcNwFlvCQ2QvlgPSZRoKJLtJpVL4fHq7Z/72hazooExBsG3rZsYAERwRTtF2V/hoem8FQQ6NwIx2khI27O6htSfGIVNG5b7c8dvsNoRctC+NoACmob/0fpn6gD5jDolEyWzHVlZvsRev796Z95BnJSY80ggGvRRqDrpjSeu7OZxQGsFe4NeEbhKqau7/5IsfhDGzAdjBaGv3zhp9nxndk2ka6hg1x75HozuaCKCpAprpoCM4hjZZwyjRmTXDz6yJ1ESn7iyOJQkR58XQ14i/cSegJ5DlChHt6Im4IpASbbo5qkc6BEFkT0b4qL4Z3Os8AodGYDjLU1Jy3HVPc+7vX8p7uXNmajrCY4k+BvnU0J3F9dhmk2A+jaCnFXas3Os2cpGrpEXpk9m8adc7H4G9rRLKFDoN0+CKtf2fN/UoqNB9BTs0W3DsbtR9DvsJPT6+OuRzOYu7m+bb9/jqsqzbhrUUVSJKOlBFK3U0ii6e7vmI65wUGpz5W3b5xwEwSezSNYJEinq6qRW9PPbcSzzzXgu/dsSjO+np7XXlPsh2fTW0CLapjEi7K3zUyiMYtLNYP3/Vlja2tuvRVZYgGKSPoMfQeqJ9CYL00MJHYxlmpxB5NIL/PQZuPnKv2shHTtOQR3kEA9IIOrboFWoLgLvceEFuOSBU+OheIoQ4RQixWgixVghxVR/nHSqESAkhzitmf7ymGzs8dOukMwB4M62vf1wdCrhMQ73N8/u8V1AkqSKKDFbTmrFMponQfLDwk9w66WcAzNPWcUbLLcx75VvcHfwxALtbd/PpW1/lthc2WNfVhG2BtPz9Ha7cB3/XVgCiBEhJI5422o50OIvN30i/pqHHf+geHIwZejQW460teoltc0bv/N3Fk2n+8Oz6rDIYznN6BmMa2ktncWZmdpBE7pli5xZr88V1u/MKp18/tprv3pu92pyFo5+52in45DgR7fPwoPIIblgAt51SgE5lmIb6ajwZ3+tyJTnbVQllg0cI4QNuAk4FZgEfF0LMynPeL4BHi9WX4YLztxtrms3s6J/4V1qfKVaFfC7TULzReFRTj855ryBJqkUEEaqhTdbmPMfn1wf0SKXu7Ls68BdO67yTqdsfZpLWAkAtesLWYvEOC4WuFYyttgWBloyyoN4eEKqj2632o+iLLnS++zSdLbqAaNnTYf04LUGQjJNMpfnzixvYYSZASQkv/MY9OKTtNRRMO6qpETh/hHct28zPHnqH3zyxxvV5XVFDhkaQOWt3kRqaj6A7owxHSPQdNbSrI8KFf3iF+5dvzXn8tU17ePX9ttwXv/cY/L9RlonJLCXipOCz1N7dfR4e1KpsA9UK++H6J97jusds7bVPH8FPm+HGhfmPDxKXGW5vLk7kLjszHCimRrAYWCulXC+ljAN3AmflOO+rwD1Afs/oCMH5g6kI+OihAjMTuTrktzLW3k+PgapG+NZ7cOFduW5FWKSoERFEqJokvpzn+IzaRYGKGnplKOc5Zu2ju0I/4d7Q1QA0h+1ZdEgkmFnRYb1vTOp/pgpixNDvX7vqr9T1bgCgraMry1kcjUW4ack6fvTAKq64+019NptrpmaYagKk2N4eYcPuHup2v8Fo9rhmfmb57cxBINcg3LdGMLTw0a6Y+zP0FzXU1qUnlezudgyKqQQ88l3o2U1vPJXbkbj2SXjpRn17m76es1k80Nlavqbf2LSHH92/cvB27e6+f5JelN7OzDDvN2qovY+ojmgntL0/4LZzFTV8bk1L3+ZHk+d+BT8bS11/iUUeUUxBMAHY7Hi/xdhnIYSYAJwN3NzXjYQQlwohlgkhlrW0tBS8o6UiLeHKxCV0zrqIo2Y0cca88dax6pAfxs3jnorzOC9+NZoAasZAsDLnveqCaWq1GDJYzVPpBVyfPCfrHL+hEVSG/HSRncMAMF1s42LfI659ldhO4zBx6uJ2ueomoQuFShHDT/YgG49FrIHBdBZ39fTyt1c2UhPy89ya3azc2mlHGRn8+rHV/N/SDYCeDPerx9/juOue5sTlX+cy/wOuQX7tLt1BWx1yB73lGhT6DB+1SkzsnbM4UyMIkuxzptjVrQ8CrhLfO1fBy7+DtU/SG0tlFwKUEv75SXj/Wf29X/fNJFLZfc430J9/y8v8+aWNyN8dAf/6Ut8fykmPQyPoI2+hlPbygOYesoYUNfTHE/TwWNA/35ZsX5yTzPDRnZ1RPvmnV3lwRXYV3yze1IMyzN/PcKOYgiBX8f7Mv9r1wJVS9q2bSylvkVIuklIuam4eQKTOMOWYGU38M3U8iQ//D+GAjxs/voDKoD6brw75wR/ibzWfpZW6/CaGD18HExcj4t2IdBJC1cQJcH3Sdq/0GP6Fk2bpUUpVQR/dMrcgmKlt5erAX6z3GmnC0h6kD2oOcuL4bLW+kpiRSesmEYtYP86AUQ8pHosRiaeYPUE3YW1tj2QVontjczvbW7uM65JWXyqS7TSILtdg86bhP+jMCG3NNSD17Swemo8gc/auF53LPzD19OhmuE6nbyNqDAzxLnoTemihS6AleiHRY7/36ea4eA4ner7vjGb8ErWWd2D5Hdb+d3d0snJrHwNTj2PSlSPXYq9KTAyx5Lff5x5WhpRHsNsRIPHGX3XB8O6DeU93LXyUtr9/nQMp2W74/7RhmpFcTEGwBZjkeD8RyCwhuQi4UwixATgP+J0Q4iNF7JOn/Nfps3juO8fTWG2bacyZXZUxuzW/5nkHlMWXwNg50GusNBbMdhTHZ+qO6JnpDYCpEeTWLDJppIMKbEFw3rwmqnq3kqoa4zqvkighkf0D0NIxa/A1S0wk4lHiyTTj6nRh1NoTy9IIIvEUKWOGbhbUq6bXeI24wkDNQbA9SxBkf56BOYv3Lq47UxAERQrZh3bR26t/HpcAixkRWbFueg1toNcpvMy/s4X+IRM5Mq7zfWUyZ9Emp1z/HKff+Hze/roEQYaNf69j6ofovA34MjSCQnltTaHw9gOw5vGcp8gMjcD0Q0UGYhoyftlaDi16OFBMQbAUmCGEmCaECAIXAA84T5BSTpNSTpVSTgXuBr4kpfxXEfvkKQGfxqRR7gH5Z2fPoak6ZC0xaQ6WIX+G3f/ob8G4+fq2MSsEd2G7/6QOAyA266P6joPPBnSNoFMOTBAcob3NR2fbJTPq/Slo30xq1P6u8xqEPnvfIt1JciGS1g/EnNkn4lHiqTRj63SzRlt3nLXb3ANcbzyFNAYJn7GGQq3QB84aEbHCQZ3s6c2tEWiOSaOzaJ+L7l3QacxLDIX03R2drGsZuA031ypuAZl/oOvt7cm+ztQIYl3Wc3OZnDIFgaFJ5fpc+WbmPl8u5XwARNuz2rXbsrcH5XsYotM4SxAUaoJthm6vuBP+882cp2RWtzX/XtEc380sjNIzucypmXREEnT0JqB1HVy7H+zZ0P/9h0jRBIGUMgl8BT0a6B3gLinlKiHEF4UQXyxWu/saH1s0iWX/dSKaMXr9/Nw5XHvuXOZMrHOfeMIP4QvP6NuOBWxEyNYIvpb4KjOjf8ZXOxau7oD9TwSgMpjfR5DJDcGbWPzsxdb7yevugO4d0OROaqszBuk/Jj/M35PHW/uDJKyZsukjiBlLblaH/NSE/bT2xPnT0++47tcbT5JOmc5i/bo6QyOoIXe0RUeve1AxB0enEM2bUPbeo4CEmafqGsHmVznl+uc44VfP5D4/B7kcuwGZf6CLRPKbhmSs05pZuu6bJQj0Z5nTR5CnXb8mso4OaPCO2yap8256hkdW2rbwgZikcjJkjWCApqH+Pl/cUZU2nXKXionlNpdl5hGYmu+ANAJDEIToXxAe/YunmPf/HoO29Xrk1iAc2ntLUfMIpJQPSSlnSin3k1L+zNh3s5QyyzkspbxYSnl3MfuzL1AbDvCxQyf1fZLfNi1pYVsQpNGIEyCYMWuqCvnoGqBGkNWfTU/o7TTPtPY58xaiBOlwlLIIiQQ9sSSacGoE+mwy4BM0VYdo7YnT0eUuZNYbT6FZ6yynmS3e5xL/fwCoIbuU9PxJ9VkagWlyqa2wnch5fQRrn4DaiTBunv7+TycxFn3QzRwkW7tjfOh/nmXD7h7X/p5IdiZ2iGTeQTYWzTYNpXr1Qae7s93a5xYEGeGk//46XDNlUM5inyYsf02uNvIOpo7BcntrhxWtBbmTq7qiCS645SXWZ2pVLpvK0ASBP0sjyNP3DIHzyMrtXHCLI2el0xHCm4q7K//GuvM4x+1tCYMzDRkRgbnMqZmYqwOmE3bZlmKjMov3RRymIS2cveZBwO+eNVUGfZaP4PHUIZwe++ng2gtU4TPMTAAt2FVVYzJAp7RNSSHi9MZT+H2ayzQEEPRpNFYFae2OkYy7Z/mReAqfsE1KD4a+z0d8LwJQI3rxkWKUI8N57sQ62jM0gs5ognNCy/h26lZrX22iBVbkCMHt2U1rcCy7euwfZoPQB7DWjMJ7/1mxndU7u/jT8+6ZWTSSLaCy6g053sSM853LgO5u1UM0X19jhzlapUJSSdiTY73naLvlJ3GSzzTk1zRrQSP9vgl2OcqJ5DJxARC3B/SASLrOc43txgi5rqWHl9e38fqmdvd9nGalDNPQU+/u5PVNexgofs393U6lZW4/Qcbg+cW/vc7L69tImgK0w07ySyRirmROZCrn4Ju5PrQpAPr0Q5kYGkHmqoOZOIVya3tnzs9SDJQg2BdxmIZ84exkskyNoDLotzSCLiryO44/+mf4Ro6aOOf+AVEzznrb7rf9AlGC9DoK0IVIopHmMd/lTNH0QS6ZMMo0+32MqgrS2h23ZzvoP7DeRMqatfoy7KjVRPiR/y+8Hv6i5chuqAzSGU3aP2ygM5LkJN/rnJZ60rwz/0ldBvde4g6FBGSih1UtSV56v93aV2GEza7Z6Z7RmlqFc7U4gFjErSHonz/uHpAdM9N4VBd+zoxoYTiLNcegaw24/7oMluQW2okcgiDf5NjvE4QdIcH/efVdtuyxBXF7JI+5wmEaCpJ0mbT8T/yA47U3XO2aBQq7M6NoHJ8tc6b+2duXcc7vXszdfg4yfQTPvNfCft97iBVb2kmlJd+99y3e2Z4dnmx1xfy+dNlmrmgsQxCArhVkoHVu5fXQpUwX20hLiBj1rPbGNJRPe3MK6O1tpiAY4FKvQ0AJgn0Rnz3w+iuzo4Z8GbOmqqCfbsNH0C0rXKUuXMw6C+ongZax1nJlk708G9ATtEN4swVBgiqiTBX2D80c9AM+QWN1iJbumOvLHUvqNdp9mBqB+4flF2k+7nsKgIvmj+IvF0xnlFHqqDOa1DM2fzyKg1oeZpTWTSVR/CQ5RLyH33A80/Ku656JaA/d6SC7e+wZWp3QBz4zT8HZP4BwwD1YxGPZgiCIvUrZC2t309Fl25v3dOjb0UTazng2fATVwh64LI3grdzJhAAyoWsXIeKMoQ1Bmg++/yvY9W7WuT4hCAt7sP/lA6/yv0Zi1hd8/2bFcw/kXK3OKQgCODQCKQm8ehO3BX8J2JqIafJymbZW3EWsxVGTK5UgmUpnmY/m/OjRvkNZDZzho35NsHSDbjr7xp3LeX93D/94dROn/ua5vILAWp/C8dlisRhZ0e7x7DUYqtc9wCjRzYW+J611v2FwzmIz5DqZxxy3ZY+tZe7aYzyPEmQkK0GwL+LQCAKGRuBMrhIiwzQU8lnO4m4qaGo0ZvQNU+kUtfw7dTh7KqfZg/0Xn2PDrMvsG1S5I4OiFXYoaYwAUWmbqkIiQSXuH2E6aWoEGk3VQZI9e5jmEBRWWWxLI8j+YQUMs9H5B1dzzL8O57g1/w2gm4c6toBMcXrb7dQbg3ktvUwRjhLKu2zn9F9e2sCu1jYihGjpsdtq9unXbt4TIZWWPPzWdl5ct9satEN+jWUb2nhujR5WmYjmMA2R4OXXlvH93/+di/74Cve9YpfB2NNpDy7moGpqBNVEaKKDcbTaA6kZJZaDYEK/7hv+e/hP6HtMEK0s2nkX/OOCrHPTUlLhMA3V0mvM3iVf999L4rW/8dMH38m6jni3NSkIkrQ1GafJyCdIS7hpyVor67fL7H/HFrj3EhJ//Zh1fiIe44hrnuKDv3qGNTsdzyOWdNW7ysd+8ff4f/7bCJB0Ceb1u3tcwmVri9u3UkWEmWKzHW3lGFwT8Vi2EzuHRpA2HMo+0nnDR1dsac+qgQVkmYaSGeFONzy5hs/c9ipLN9hmsp4eow9KI1DkxOEsDlfXUR3yM6E+f1RQVdBvhY92yUoOnNikZ6iOnsUXx/8fX018jf8c/S/7gtEHMfWjP7ffO5fiBNLVY63tqAzS46xECszX3FVZpSEIRkU20FgheDR0Jd8P/N063hvTj5sCwJrF56ChVTdHTNl0n35tPGWVQojIoFU7qU70MEo4ZnUOjeDmp9cRJk5EBomnbaFZldbPjyVS/O+z67jsjtf51J9eZXu7odH4NT7+h5f55J9e5Z3tnSRjhiA487esOPr3gC4Ijnn4JH62UxekaYfD1ekoNGfPWlwf0KtEhGXhy3gp/FW6Y0n29MTpwLHyXQahpD5bPE5bTrPoRJjmtES2cIom0i4fQa3oYUdHlGoiVIoYTXSwrT3HrDPeY1XODTo1Akckk1/TSEvJLx9dbTmTb39hA/N+/BiRNt0hW52yZ/q7OrqNcueS9Tvcg7UzIugfr25iV2f2rP7E3of4lP9xvuW/i3BAc5nDXnHUadrW2m4fkJJP+h7nX8EfEksYfwOnRhqLQypjsI1nCwJplHLxkyKdhmg8xk/9f6I2qvsbEqk0Z/72BS6+bal+QXcLdBoTHlMjML4DCUfp8+5Ykl8//h5LVrfw4Ft6SPOY2pBjsSflI1Dkwgx1C9YQCoV56tvH8okjpgAwpTHb7BMOaJZfoIsKDh5fB6FaCFZZ+QuZZg+nKYiwO5Q1VW1XCrn6nEP4yucvYXnzGaw7XQ/6Ok5703W+TyYYRytHP/phjtr4O8YJ9wDwxnr9y59pEspF9dbnrO0GOumNp0h06ANOR9JPtTGY19HDxGAPcenjbd8BSIdGsHjaKCqJ0UvYWiEOsLSJWDLNIyt3UBPyk0xL7n1Dv38imSaVTqGR5rYX3rcd3tVjmDtDryIbdAz2QZ+G5hiYQyT40GxdmzIjnvyGCaLaESK7blc3C3/6OOu37iQZqoPzbOe3da9EJ410cJCmV3Fp1AxTR47ZYySRcgmCGiLs6Y0z2ljEqEl0Mq4unHWdUxC4nMUuQSCyfBOxZJqOSIKnly7PumXM8JOcqz3HMf851i5MiG3/b+mK8d173+Ksm17Iut7UbE/3vZz1nX1xnd2vSK87PLRRdFIpYiSi5nOyn3c8Hst6bpu2t2RlDJsagV/oBb+rOjfwCf+TzO19BbCF+2sbjVn9dfvDrw/Ut43fU9AwDVk+nmScPe22oNzWHqUm5Ke+IggpFTWkGAgn/ACA0TVhzpw3np+dfTAPfPmorNOEENz6hQ8Cuo9g9vhaOOnHcOgl1o8pSxC4b+B6294439qeO3UMC6ePZf6X/0bzwcexOd3McT63IAiIFBOEbk4Z2/pq1u1/dLde4yXTSZyL0GY7E/YD2tt0RRNseF83SURkiMqUPiutEz2MD/bQRi1vx8eQ3L1e92q+dTcyGaNSxJg7daxLEJgFwbZ3RFmxpYNLj5nODfX/5BM+PdO0rTfO44ErWBq6jA2tvaRNjSAQtrQ0Z9mNoF+zbPkAx+9Xyw9O16vKmvbwYNIUBPaP/d43tiKlnr39XsUCmPUR0kd8jZTP1voqkp0crtnCrcnUfqLtcM/n7cqq6IKgQrg1grSEA6p6jGs7WLOzm18/tjq7vIWlESRsk4cjpHWetjZvtNLG9dlrXMRi+uecKFqoSOxhjFHi/ECxiYDh29pjRINt74haZkOTQFofsJvpoM6f5L/9f6DBiCZ7Z3snVUbJlkiv00EdtwIBkhHjOTkG/kQi2zT0iweW8ck/vuLaJ82kMJFGSok/qgcgVBnaWeZqfi7y+Qge+z4N911ondYZSRAK+AgHNERKaQSKvphzHnziXlh8qbWrriLARYdNoa4ykPOS8Pg5RGadz4QFJ7NwSgPMvxAmH2YJgIq+BEEGoqLefuO3Z5IVAR8r5LSsGX+IBOOFPlvzBbOroJqDlF/0HVoHICJtsN8JpP2VLNJWc+lfX+PpZXoN/0oRtcxLdXTTrHXTJmvZJevx9bbAjrfgns8xu30JAIsPmGT9QAEO095lmtjOK++3Ukc3J1ev48zo/fw0cBsAa3d1s5+2nUbRxZbdXZycXGI8gwrrOZimKYAaX9Jlqqnxp5hQX8HY2jDLNu6BdIpQspsIQTThHkynN1fRFEywvkOSQuPZqV9lTcL21YSTnXxAW2W9b9QcZrC3/g86dE0hvf0tksmkSyOoM/o4wa8PoKPo5LWNrdzw1FrL+YqUunmkUl8W1HQWt/XE2bLVriX5N/k9DpDrc/6tqmPZy1zqjlnbRDKGNhaK93gkdBUf2P1PANe62UtWuyugBtJR6/rzUg9yoX8JV1X8y4romj1e115jztDeVIwKYQiCqPGcHD6CZDzbNFQlIlZNK/s+5vdU9xGEDEFQnWoH3GHB0h1fmyUIrDyQ1nX4OzbYfUlLwgGNUMCHMPukfASKnPhDsP8JWTP1PgmEqfjYLVzx0eNds/9wQDNeBy4IwgGNLrOIncNfEfBpdMhsu3aAJBMNjcDvy26nkQ72F1uy0u9jZ9/GmbGfZHegdjzxcYdwqLaaVFpas8pJwh40bgjexILIS7TKGnbJejSZtOrJ1MZ0u60WrCQUtB3dB2mbWBL6FtFEmluDv+SAh893Nfuew7l5fO/DnC2MLORAGAL685iqOSq1+nsRDh9BlS+JEIJDpjbw+sY90L0TjTSbhHtxeI005x0ykRoRpTUR5I1Ne2jribvq1FSkuzhCW0VMBoxnmBHl0r0Ldq9F+9+jdHu6I3zULD0+2qiE6Rdp6g1taMlqo75QMqpXZTU0ghBJkmnJwp88zm2Pv+ZqyhTymYxK5hIERk6JMSCOE21MEPqAOqn7LcBdQ+qpd92CIJS2Z8cTpH5/ofms/IKZY6upCPis5D0AUgmrom4qh0YQT8Rse7xBNdmzcDM4ICDSpCVUxPTPHUq0c+Q1T7kEWLsj2XHtxs28u71d/9wiQS09JONx1uzsYkfLLrSEO/os5NcIB3xoSiNQlApTE6gI5vgqjJkDB5+btTvk9/GR+P9j3X6fhKrRrmO5QlNdgqA9e/Z4e/Bangh9J6uaaWj8bN6RU1zmGwAqR5GeeBgHiY1U02sJglEi28E3fvxEmsdNBuD5l/V49bqEMdgFKqkMBbOuAThEW5O1b2enPXiYn0f/UBVglPrYX9h1Fc+Qz7hmxZWaPmM8eHwdW9sj9O7Wk8U2+ae62nn+8sVcevR0AukIUVHB4+/spDuWdJnOpiY3MF3bwYtp3dTUoLkFwfatG62icSdprzGpxp401Gn6wGL6CAD+O/AnQsR56l2jv2Z4pekjcNjyG4S7Lef7uWIdS0NfZBSdWZoh2FnmZvTMGNFmrWsh0vrAZyYKHjKlgadX7yKdlqzc2sGenjghaf8NxkpdSEgtyGeOnMbRM5r42gkzqKsIEHcIgq/f8YoVNZWKmpE4To0gka0R5Chrsnar/jybq/ykpaQyoQuCRrrY2h5hW4d9zcY2u/0v3fIwvRH9WIgEK8KX0PzQZ7n4tqV0t7fiT/bgLAESDvgI+zU0s0/9rBRXCJQgKHNMTSCryB3AZc+7HZWXvQSXPkM4oLFOTmDtwv+CjMqWucpd64JAn/WJrGqadkbv2MyZZWUTT1xxIqLaLWyoGIVv0qH4hORAsYkZofa8n2+/KZM57zh9fWgzcqg+aQqCCsKh3Ka0bNymm7nCIdACYQjqGd77C7t0waWJv/LBtjvtj2MIgroKvc34bj2beFXloa57jw8n8cskIhWntq6eZRv20BV1C4JjU3q5hCfShwAwKkMjuOk/L1pO3VGiixP2N/JNfCFqDUHQhB2qeKpvKSdpr7FmV7dulzejZhzOYpPMtpzayBf9/6ZZdHJuwzomCvdsHoDeVu4M/sSKLBvrWAY1nYixvqXbcqSfvWACu7vjrNjawek3Ps9Ff3yFEFHaRD0AzSlDEPiDfPtDB/DXzx3G6Jow9ZUBurvtScEbG1os01A6lh2Suaerh1Qi0zTkHnyjiRQbd+qCrVLEkRKqE/p7UxDu7LCv2b7bNis1i3ZLkJomuqqNTxJLpqgRvWikqfXFmShaOFN7ge/2XEs44MNnCEaiHfq64EVECYIypyLoc732yZhZMH6+JTQyM20BK3HNSUCk3DH9eTBNBIBuU61oYEpjFVpGHgMVDQTHHQTASb7XGJXYQSqYe7lOYl2MGa9HVI2J6TPwxpTRTrCKqhwagT9HGQDnYj0Ah2tvOy4Igz9ISgTYz6ERAIxK7iItfOyR1VQYdvGqkP78ulo2ALCuxi0IiHdbA3FFVR1b90SyBEEFMdpkNa+n9WKADRmDc7NotzSCJtFpFzurHk2NMcg1pNv16DGDBp++qNDqnV1ZGkHQoa31pRHE0fNZDqruoVl0ksmMLfdyuPYOczW9XMc40WY5cvd09vDBXz3Dul3dBH0apx48FiHg/5bpPom3t3cSkjEi1bqG12iYnqRmmCelhJdvZlKwiy27bQETIGnltkhTwDl8BH9/eT1vbnALLTOKy8wqX72jC7/pn0hHSEtJbUoXBI3G59zRGeXjvieZInbQ02VPaprpsEqr1zu01qqgzyqoeFb4TZ4PfZ0bgjdxVOxZwgHNFgSrH4RfTMl6loVECYIyxxzMB+MjaKjSB8+GyuxBNFeV04liN1O0XSSaspasduEaOCpG2dqG4bC0qByFqJtErwzxBb++kIhv/sdz3zRQAUbew3T02XpTerd1rCKsDyJPp+bx/cRngewZL0BjxspSPiF5NX0A2yedbg2WCX8VPpEdQbODJnoIW6GlU9pe4N/B7/HMS6/ovpaMPA1i3dZAXFldy86uKO29cTQjv+LV9AEAbJVNdBs5HFVp96DbTDvSsZ5AVdwYmKqaLEFQLbtgzGzrnEMbdHPGrc+/z65W4/wM09D+zVUu85f+bOy2Y0Zy4Sypm9Z2+fVn3+HXP+OY7rdd144RbVQas3Xz+fzfa1uorwzQWB1i/qR6HnplJbWGD6OSGJFQE/hCBI1Kr0Gf8czb1sMjV/Ltzl+4o7dIWqYhGe9BSklbZxdRw7/iJ0mPM9wUWyOY++PH2NERZdW2TuueoXQvG1t7qYgbGgFdgKSlo5ufB/7Ex31PEe2yBVGzaLdMYU6BvX+g1frsH9Lc0XSVfomvj0q2hUYJgjJn7sR65k6so7Eqt608F/Mm1vHvrxzF3MxS2eQ2DQFsl6OIfODbA++YUwvIHCgNIfE++lKfu6tm6Iv1GHTKCrplGM76HZx4NQQrSQaqrUHaSjQLVFFpCII4fnZL/fOMdpgrTJ4LXc5Msdm17/HwKXDeH606NUm/7h8xHbgm65LNxGSAilX/hGW3Mmvp95ijbeAk32tsl6P0eb6zDHLcnpFX19YjJazZ1W1pBP+TPI+HU4fyy+T59EpdEGQKr2bRQaLTnuXWd7yray6hWqoNZ3FVuhvqJvL7Y15hq2xkakAf2B54cxuX/033p2yO6vc/sDnEby9cwA3H+5ihbeXx1CHWvc3B7U+fXsTYBj1YYGqn7lBeiV61tieYodUZjGWPpW05tY76ygBseY1fJn7OzcHruTbwBwAqRIy0vxKq7ez2SjN3w5jtN6Z2u6KkHg59l/01Q3jFe/jbK5vYuLPN0l4DpAg6TF+9MsT+YitPBb/JAam1bNz4Pls3rqbar58TSkeMZ9wO6M72Wnroate/N+O0dpI97a6/RcjIjG9waAQLpR31NSflFpBXrPwIx+F2yhdzcWglCMqcxdNG8cBXjhqURiCEYM7EuqxSFkBWlnFP48E8nZrH5+LfhgNOhdOv5/mz7KSwhMzTbqVj4JiakRthaAjmDG319IvBLL7XdACLY79jUez3sOAiKxlOVo8ji0AFdYb8ixOgxRAEM8WW7HOBUzNmbd//8qXWQkKgawQAm6V7OdVNcgyTTXv5638lHdLbGSPa2SabeHtbJ3zpJVj0Of2cmG0aqq3TZ+Srd3RZgqBdVnNZ4nKeTc+z6jyNyjDDNIt2utvsMh6VXet17ShUYzlCK9NdEK4nnvaxTTZSn9jFh2aPYUJ9BTPr9b/tVQ/pwq9CS3H65CQznv0acenjztRx1r1NwTrHt4lmqQ+G4XgbvYR4K64/92Qwe9IAMFbbY5ltnJU5I4kUrHuS/fc8xyKx2np+YeKk/RVQZ0dazYu8DI9+385vSCfylnsW8R4eWrGdMHFr0uIn5Wr7jfT+zNF0Z/zx2nIOue8Yrnj7PEaHjfUu0hH8JGmigy3G3/q7/n/QY2gB43wdJHvb7c8o2gj79GudZrSFEbssdl3aPh+gKpmjIusQF/XpCyUIFAXFpRGc/j9sPOteLk5cydtyKsFgCBZ9hqMWzLVO6fQ35rgLUOXYv+hz8Pkn7fcVuiC4Jn0R96c+wO7pZ9rLTU49irMX78/Zi90rqvmbpme3EaykJqD/QGP42Y0+WM3WcpR+Ri+w56LOHfaZ9OsO41bc/oqNcrRVK4ltbxDqtO//cHoxPz5zNjQfAEd82WioHZ7/HwBGNeiftTuWtMJH50wbxwFjapgxupooQdJS0CiyNQLRs5v30noWeCDSYkU3VcoIgjQVqW6oqCeRSrNdNlIb38nNnziE5688ni8foT//Db0hEtLHYZOrYdW/CLSv50uJb7DNsTLdKDq5zPcAo/9+IrO67UqiG7TJtKX0iYFzJT0nAZKWb6hKRGkw8mA2t0WsCqE+IS3TXCUxZKASGqZa9xgf3wAv/RZajdImaXfehBOR6OG1jXsIkrAmLQFSLm3k5fRB1vYc7X38holmWoUuQIPpXsaKPfiEpHPicQB83L+EMXHd+T9GtJMynLvdvnqm+NoImhoBtkZwROLlnH3MS7yn/3P2EiUIFAXllxc5Zu+LPktlpT0AZJbHBmhsbM7YY2gZTo1ACJi4yH5v2K2fTs3n64mvUFMRggNOg6O/DSf9mJ+fM5efnzMXJ6Jxv+zOBioZFdLV7ZgMWKahWSK3IHDawpl4aNZx0zSUmUuxzTfBiioCiXAsznLQhy7lmJnGMzBXm1vxT3hXX5SncZTtHzEdjqMbG3n08mM49eCxSDQiGQJqR3AyDXSjRVpZL8cT1wzhbEQ3VRChmigCCeF6PnPkVEKNU6hLtCCkRAhBndQFSxs1SF+A5goB7RuR4XqekodYTmGAyVoLV/j/mfU83vTPI2Gcp+URBADTjNyLiWI3V9XpWdwnHjQGuuwAg1F0IUhTKUxBkMN5ulafLNQGYUyeArvJSBfxVJqwiFumIT9JgiRZw2R+kzyH59L2d2eRttraHtOhZ8yH0hEmoAuvmcd9nLtm/RaABUbIcaPcY1WV3RyYxnjRavlYzOTJXRNOyurbGt/+Wftc5Kh/VCiUIFAUlP0muU0wlY5oJE3LkQBnDhBmhq+ZqZwZKeTErw985upUNeGAPsid8AN7MM1kVA6NIFDJ5Dq9f3EC9BImplWwv2aYhoLVEKiytBHLYfzh6+DT/8m6nakRtMtq5Gce5pcz/sZl8a+zLLgYvrYcvrUaqvRB/9L45RwUvZWaKseIZQqLVjs0NVRZy+ga3fyTNn6uY0fpAstcrSsijGcWrGHXpW/RPfNcKkWMcGQ7rbKWZLXuS6FuEoRqqEj3WiW3qainsTrEh45crJc06NwCax4n0LKKKEEihPDJlD7jXvpHRP1k6ioC7AxOgbNvgSO/DhhrV49f4HoeK8ILrQgsf44FlExT3P6OJLzz99zCih+ewE0XLXCtGRAQKUbTbrypgPocguD9Z/XHQIozD8wdRWbmEYRI2KYhkSJAkrdTE7kx/VHekZPpkhW0yDqXTd/UOv2kuDF4o77dMIUjj9SXaj1E6AmL1bKbipguKN5jCo2yzcqINtk57oNZfXuuInufixwVUQuFEgSKwpIxEOcNS51xMhxysX2+6QcwE30qcwiCjOQ103dWE/Znn5tJY47ZVqDSqucSN5Kaov46O3pp1HSoqIeJi0iKAM0YgqBmnC54MjA1gk6qEFM+QGfVNB5OH0Y4FITqZqgZC5e9iDz62zyTnkeEMLUVDsdyoEIXiJ0OH0VFAzPG6IPohfHvc3PydMaO1h2lZm3+uLBn/KPHT7ZKgISS3eymlnTQeMZTPgChavykGG3mEJgFBU1n+5alcMd58Nb/0aXVAgKfdNjb6ydTXxmkoSoA886HI77KlYlLOC3+c2i2TSoA68MHWzPhYDh7ir5J6n/PRnOAN6hdcRuhncuha4drv5XEl2EasjAzdGMdsOttS+g6EfFuJooWauh1m4ZEgpgMMKY2TIwgH4r9govj38luw8BKxqubQLiumZ2y3pWEOCa+CSk0Xu0ZjUYav3T7LLbWzMu65yvVJ/Kx2A/ytknX9qI5jJUgUBSWoHvmVxnMM0hf9H9wxm9g4af194de4j5elcN38OVXcq6g5lyLIS+5TEM+v+WAM00dsYBjJrn/CTBBj46JaRW2aSiQOzIqremaSsJwBgeN0FzXM6gejTjhB8QMc06dUxAIAeag3XQAfOE5qBzFjNH6vnfkFK5JXsikRt30FDDCaxOGADK1Ka3KXkq0VdYS7DVm1pOPsPIGrLIQ4Xr9dcxsQMDKe61rI/767A9ZP4W6ioAdOlzdzPSTL+PkhTNtn8kRX4Hv78QfDPFsWh/w5MHnZN1qm2y0Crm5eOQqePG30O3OPTEFgQjmMQ052bHCXo/aQWWilceDVxASSUsj+ITvCcaLNuL49fLPwDaaWCWnWcEMqbHGvSYeynP+w+0bBiqoCPpYnZ7kKuw3NbWRLlFDd3h8zu7tSmRPJOLBetciT1n87Rx47L/6/Nh7ixIEisKSseRf5mppWcw6E364Bw483d434RCYsCj73MpR+gpqGdSEB5AdXDsRwnX8uuIrnBv7Ee8d+zt9/+JLYe753JLU248F6gHoEVV66On5f9X3iwqaTNNQILcBWpP67Fcag6uZo2EmkOXsVkWGEDML+jXNgHG6rTqztPjEBsOkYWgE6QxB4HcJgjo69zvTuHCRJait5D2zvWCVrjW9a5u8EkH9WHzWeXbjoWo+e9Q0PnfUNGvXF47dj199bJ7luyEZhUCYkN/HGjmRmck7qZhqD55Jof+9Tl0wHWHkeGyonMOn41fa7bS8q68d7PzcZnZ6oBKqx7J9wilZoboucizsMyW1yRqwTR/BQZru5E3gZ0ytPUBfcvQ0Nn/yRV4+9Df4mvTEPRr3Z/Y3HnDdM+z3WdqNyXS20ipraBg/jVzsivt5ITXbtS8U8BPpSxBA3ybTIaAEgcJ7NE3/9/kndVv6JU/1P+NzMCCNQNPgqk08U3Uqr8kD6Jp+qr4/XAfn3EKnsQhMwghz7NHcJq6IqKCRvjUCzRi4KgwziJmB3VdorksjAJhnrDDmmCmbawUcPaOJ575zPFXG5+02q10GDee0JQhsbaqVWtqP/D5cuUE/zzDFjTcFgakRQNYMuq5xLOcvmkTgo3+E8/+m72yYypnzxnPW/AlkYZr5DFu2KQiDPs0qbggQCer98wftMNCYFqZVOp75LiOu3m8/68WaXiJEC1WBpvHO0TfwrOnYbT4w+zOMn+/qXkL6XDb/zJyXGAHGOxZ4OveQiUzf/0AOP+1iqDM+rz/EqOoQnH8HXKgvJappglbNnfQYECl2pmoINGRPXJJSo6UHPpf4Nq+e8qC1PxTQiMj+BMHovo/vJUoQKAqPLwT7nzj46yYuglG5Z1C5+PdXjuKKDx3Qv9bhbKJBH6QzTa3PXnE8d156uDUL7vW5nY1REbZXTsujEaSNmvaVYd1sEjIGP38f/avN1GY+8DWYfryrxPixM0fzwQNH81+nzWLSKLvtnV26AzJZZawYZ/gtgjW2INgtawkGQvZs3XDOjzcLwjlLii/8pKsrTY2j+MV5c/V8kYPOgM89AfPyZHCD7eeZo2sQliDwa66cE2n6f/why2QXE2HayeHoP/pbcNx39efg08uNJ4Q+WIb9PqKGb4cpH9CFZ/NB+vn+iiyNYLs2xvU+M+cljp/9mm3TZsAZ5VZrmL1iRpjuQafDzA9Zh9t99jNPGxpPq6yhvs6RP1GrCxO/SNMWSRAlhDZ2lm4WPfmnhPzZEWBZVI/p+/heMoCplEIxSH6Qo9hYEZgzsY45ObKb++K/z5nDIVMaOGRKg2v/5MZKJjdWsuqFegAifrcgiAjH7DGPRhBJ6oNdRaU+QzfDZX1a/vlWZaYzPVQNn/qXa1dF0MetF2eHq8bNVa5G7QfbgLSxtrJDELTKWstXoR/UP9d0sQ0pfAinT2fasTDlKNj+pp7dnFkHf1J2H1yMmgZX26U4TI3IuQQlQKBuDOxZCb6gFc2VkoI2mUMQLPqs7i/a/R6svEfvxuh6AMJBnx3GWj1WFwATDtF9O0d/21VhFGBHYCKT43Z5jMy6WBKNUY4Me1e4s6kR5CiaCNDpa8TMSUtVNaN1b6NV1tFcE9LNcfFuXWvp1MucmBVWq8N+OO06/Xn9ayW9mcIp2EBw1odh+R3G58x2gBcCpREoyoq6igCfPWpazqxogJRhKolmCIJelyDIrRHc3/wFbk+eTMeUUwBbI8gcCJ3k68dAuPKUA/n6CTOYfqBhHjHCLSuqG0hLQQpBO9VuQTBmNnFfFftp24nW7ede00II+MyDcMb1+vuEeyAdLObnD2YUJwxXGs/WH7IEQU2yLbej1Kwzdd6tcNE9UNVM7QS9ZtXMMTVMrDOuCVXDsd/RhQDogQA+x+w6WMOjVR9x3TqQUVywkQ5XIUWXRmCu051PEDhKaPhq9RDqNmr00F8zMmu0HVXVGdHbrgzYc/GQX7NKcpv0VE5k7RGO9cOVaUihKD7psD7wuKKHgJnO/Ig8GsERcw/k6uTFHHmAfq45I85lujpn4QSaqvuxB/dDY3WIy0+aic+MiDIzcf1+uqikTdYi0dwDsT/E5qajAeiYeV7mLXVMO7vD9LE3mINqICORUJgBBQ6NoC7ZipVMaJ/pFlQzToQr1lqz4uqQn8MmG1pErvwRzTGofm8Layvnuw4fPtZtHxzr63D11e8U4LVmLsbk7HaA3pA9U9cMQbDb1AhMQeAIYTaXtax0BBLoglO4liTtqpjAdY+ttRsqkrNYmYYUReepbx2bd13b4UY6rJuM4gG3yam+rt5+488O/QM47oDRbLjmNOu9OYzk8hH8+mPzh9JNNzn8Kp2imu60PiPOzOheP/lctO1vwKyP5b5f0wy4cqM9gO0lppM8K6PcSh50+ghyCMVgVfa+TMzSIhlhy4BdvdYIyfUHgtyW/BBzqjtYFH2Z049aDPffbp3e66ujNp9GUDsOPnkfjF+YsxuJoMPUWKNrD22yhtE1Ydsh7xBWncb6z07ToDlxSAcq0dIxPhP7NjNHn8TjS3diuQ58A10/Y3AoQaAoOtObc/xIhytmSWnDaWxhZkD7w1mL8eQjZSxQ3pePoCCYA4zDOdou6mkzIlAyTVMVMz/I6a/cwIuj3bWS3CfVD71bxqCaZf4y3/uM2fJHbqYzeDAT/92Ca2GwPJqXC1MQ5BHOfPTPVvTQYdNH8eN3P80BoRoe/WS1ngl9/5cAuD55Dg0nfJPJDkGQJcD2y5/5Gwo6BmjDjNQq63SfgylQNR8c/S1+tmQHvckUQugOb+seRttpfxghq3k3cDhPv+JYCa+IKEGgUDgxojLioYw1EMwZZ31u00AukoYg6MtHUDC++a4trID/qfwa69piWRE7AEfu38iKqz80qGirvcGc4VoLufsrjBmt0a653vX8jzMLeH4WsOUp6NkF/7hAdxT3hykI8s2UZ3/E2rzk6On0xFLMmVAHE9zRN5d95UqCYw7g7e12PanB/N0qAj5+r13AZWceh7ma3eVnH6k/Y1MQxHvhhB/y5yUPA2kqAj5X2RVTEMhAFYg0BzfVsaMzyoFja8hIvi44ShAoFA5kw1Qujn+HD4w92X3AiMjJKondB8m0HtVT7AEX0E0XDlrCU9koO6jJUehPCEEpZFMokNH2lRv01/9crr9qOfIrJhrrHFy1yc6y7gtTEGj9D2VCCC4/aWbuvlbWgRAOLWZwf7f9R1fzQvyzXDb/MN3J7g9z+MFH6AdN7cooRBf2a8ST6ayIsZCZbxKoAE0yc0w1T7yzkwWT6+HsR3KWNSkUShAoFA5Cfo2n0/M5JpRhljBrwe+fXTUyH2aW6n4emMbMJLvMiJ1SYpk6TI3AHMhMDaUvv9FA/RMHng7rn85dQmQwGNqU6RcIaNmaVF9855QD7TeBCnCW1DBXgTPMjnWVATqjyazyK4dMaeCYmc0EqYaE4KBxesDC4dMbYYq7mm6hUYJAoXBgDpxZs9mjLtd/0AecOuB7nTxrDHd8/jCOmJ5nzYUiYhbi81YQGM7PzPHeEgRphsyhn9ezsfNVnR0oASP3w4p0KqDKtOCTeqFCI8myriLAZiJZGsHMMTX85bOL4c1PQTLK6XPHMbGhgvmT6gvXlzwoQaBQOKivCKAJaKzKiGIJ11oZswNFCMGR+xcn3K8/zPpLw0ojMLHKZxQgkkyIoQsBsAIATAdxoJDPTQiYYWuSZlmRrGRCE6PMiAAWTG7IfU6BKeq3RAhxihBitRBirRDiqhzHzxJCrBBCLBdCLBNCDNwAq1AUgdG1YR79xjGcNKs4qfylwtIIcvgISoWlVWWN9wXUCApMIE/uQyGxBcHwmYcXrSdCCB9wE3ASsAVYKoR4QErpXKX5SeABKaUUQswF7gIOzL6bQlE6ZowpwAzTY2oNQeD3UBCELdNQpkYwfAWBpREU0cFvCoK8a3V4QDG/JYuBtVLK9VLKOHAncJbzBCllt7Riy6iiILqiQqEwTUOptHeDrakRZP2oD/+yvlragWeUvE/9URTTUAbmYkRVw0gQFFM3mQBsdrzfAhyWeZIQ4mzg58Bo4LTM48Y5lwKXAkyePPA4boWiXDFNQ1ZhOg8I5dMImmfC5dkLDHnChEMgaS8oo2kCvyZKYhrKuXSrRxRTI8j1KbMmB1LK+6SUBwIfAX6S60ZSyluklIuklIuam4tTfU+hGEmYGkHMU0FgaATDWc+/5Cm47HnXrqBfK6ogMEuPJ1LD58EUUxBsAZyrMkxEL5abEynls8B+QghvwiwUihFEtaEReCkIgvuCIMhBwKcVNRvcNA0lU8PHR1JMQbAUmCGEmCaECAIXAK413oQQ+wsja0MIsRC9tFLuOq8KhWLAmKahWCLVz5nFw3RU7ysFB02KrRGYmlJiGAmCovkIpJRJIcRXgEcBH3CrlHKVEOKLxvGbgXOBTwkhEujlps53OI8VCsVeUjsMNAKfER2kDWHNBS8IFlkjMO89nExDRQ1klVI+BDyUse9mx/YvgF8Usw8KRTli+giSWWm9pWNMbYgvHDudjx7SR5XTYUixNYJJxnKpCybXF62NwTJ8MhoUCkXBME1DXiKE4LunHtT/icMMXSMoniCYMaaGJ755DNOahk95du+/LQqFouBUBIZPjPq+xmeOnEp9ZT+LyA+R/UcPr6RFJQgUihGIEIIfnTGLQ6eO6v9khYsLFpdfrpISBArFCOUzR2YvYalQ5EItXq9QKBRljhIECoVCUeYoQaBQKBRljhIECoVCUeYoQaBQKBRljhIECoVCUeYoQaBQKBRljhIECoVCUeaIfa3YpxCiBdi4l5c3AbsL2J1isS/0U/WxMKg+FgbVx/6ZIqXMubLXPicIhoIQYpmUcpHX/eiPfaGfqo+FQfWxMKg+Dg1lGlIoFIoyRwkChUKhKHPKTRDc4nUHBsi+0E/Vx8Kg+lgYVB+HQFn5CBQKhUKRTblpBAqFQqHIQAkChUKhKHPKRhAIIU4RQqwWQqwVQlzldX9MhBAbhBBvCSGWCyGWGftGCSEeF0KsMV4bStynW4UQu4QQKx378vZJCPFd47muFkJ8yMM+Xi2E2Go8y+VCiA973MdJQoglQoh3hBCrhBBfN/YPm2fZRx+HzbMUQoSFEK8KId40+vhjY/+weY799HPYPMu8SClH/D/AB6wDpgNB4E1gltf9Mvq2AWjK2HctcJWxfRXwixL36RhgIbCyvz4Bs4znGQKmGc/Z51Efrwa+neNcr/o4DlhobNcA7xl9GTbPso8+DptnCQig2tgOAK8Ahw+n59hPP4fNs8z3r1w0gsXAWinleillHLgTOMvjPvXFWcCfje0/Ax8pZeNSymeBtgH26SzgTillTEr5PrAW/Xl70cd8eNXH7VLK143tLuAdYALD6Fn20cd8eNFHKaXsNt4GjH+SYfQc++lnPjzpZy7KRRBMADY73m+h7y97KZHAY0KI14QQlxr7xkgpt4P+QwVGe9Y7m3x9Gm7P9itCiBWG6cg0FXjeRyHEVGAB+ixxWD7LjD7CMHqWQgifEGI5sAt4XEo5LJ9jnn7CMHqWuSgXQSBy7BsucbNHSikXAqcCXxZCHON1hwbJcHq2vwf2A+YD24FfGfs97aMQohq4B/iGlLKzr1Nz7CtJP3P0cVg9SyllSko5H5gILBZCHNzH6Z49xzz9HFbPMhflIgi2AJMc7ycC2zzqiwsp5TbjdRdwH7pquFMIMQ7AeN3lXQ8t8vVp2DxbKeVO44eYBv6ArWZ71kchRAB9gL1DSnmvsXtYPctcfRyOz9LoVzvwNHAKw+w5OnH2c7g+SyflIgiWAjOEENOEEEHgAuABj/uEEKJKCFFjbgMnAyvR+/Zp47RPA/d700MX+fr0AHCBECIkhJgGzABe9aB/5mBgcjb6swSP+iiEEMCfgHeklL92HBo2zzJfH4fTsxRCNAsh6o3tCuBE4F2G0XPsq5/D6VnmxQsPtRf/gA+jR0SsA77vdX+MPk1Hjxp4E1hl9gtoBJ4E1hivo0rcr3+gq7AJ9FnL5/rqE/B947muBk71sI9/Bd4CVqD/yMZ53Mej0FX9FcBy49+Hh9Oz7KOPw+ZZAnOBN4y+rAR+aOwfNs+xn34Om2eZ758qMaFQKBRlTrmYhhQKhUKRByUIFAqFosxRgkChUCjKHCUIFAqFosxRgkChUCjKHCUIFIocCCGmCkdl0wGcf7EQYvwAzvnt0HunUBQWJQgUisJwMdCnIFAohitKECgU+fELIf5sFAu7WwhRKYT4oRBiqRBipRDiFqFzHrAIuMOoN18hhDhUCPGiUZv+VTODHBgvhHjEqKF/rYefTaGwUIJAocjPAcAtUsq5QCfwJeC3UspDpZQHAxXA6VLKu4FlwEVSLziWAv4JfF1KOQ+91EDEuOd84HxgDnC+EMJZa0ah8AQlCBSK/GyWUr5gbP8NvRzD8UKIV4QQbwEfBGbnuO4AYLuUcimAlLJTSpk0jj0ppeyQUkaBt4Epxf0ICkX/+L3ugEIxjMmsvyKB3wGLpJSbhRBXA+Ec14kc15rEHNsp1G9QMQxQGoFCkZ/JQogjjO2PA88b27uN+v3nOc7tQl/qEfTKmOOFEIcCCCFqhBBqwFcMW9SXU6HIzzvAp4UQ/4te4fL3QAN6JckN6OXNTW4HbhZCRIAj0P0ANxrliCPofgKFYliiqo8qFApFmaNMQwqFQlHmKEGgUCgUZY4SBAqFQlHmKEGgUCgUZY4SBAqFQlHmKEGgUCgUZY4SBAqFQlHm/H8l4Y2eaReMaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "training_loss.pop()\n",
    "validating_loss.insert(0, float('NaN'))\n",
    "plt.plot(training_loss, label=\"train loss\")\n",
    "plt.plot(validating_loss, label=\"validation loss\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "for i in range(epochs):\n",
    "    plt.axvline(x=(len(train_dataset_X)//batch_size)*(i+1)/200,color='gray')\n",
    "\n",
    "plt.ylabel('loss');\n",
    "plt.xlabel('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6e7c0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data_loader):\n",
    "    correct = 0.0 # here you will count the correct answers\n",
    "    total = 0.0 # here you will count all the answers\n",
    "    with torch.no_grad(): # ingnore the gradient graph\n",
    "        for batch in data_loader:\n",
    "            batch = torch.tensor(batch)\n",
    "            xs = batch[:, :-1]\n",
    "    #        print(xs.shape)\n",
    "            ys = batch[:,-1].clamp(max=1).long()\n",
    "\n",
    "            hat_ys = model(xs).detach().cpu()\n",
    "            _, hat_ys = torch.max(hat_ys, 1)\n",
    "            correct += (hat_ys == ys).sum()\n",
    "            total += ys.size(0)\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "1c24bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/66fq_8ls2qvdfwvlvcw1hxgh0000gn/T/ipykernel_86887/796367905.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = torch.tensor(batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the MLP 0.843\n",
      "Validation accuracy of the MLP 0.843\n"
     ]
    }
   ],
   "source": [
    "model.eval() # set the model to evaluation\n",
    "\n",
    "train_accuracy = accuracy(train_loader_X)\n",
    "print('Train accuracy of the MLP {:.3f}'.format(train_accuracy))\n",
    "\n",
    "validation_accuracy = accuracy(test_loader_X)\n",
    "print('Validation accuracy of the MLP {:.3f}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cafcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b741167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
